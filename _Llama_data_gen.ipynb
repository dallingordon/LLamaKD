{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110caed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/projectnb/textconv/llama/packages')\n",
    "\n",
    "import fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ca260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projectnb/textconv/llama\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350fdb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca58b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama, Dialog\n",
    "from llama.model import ModelArgs, Transformer\n",
    "from llama.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0570a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 1\n",
      "Device 0: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_cuda_devices}\")\n",
    "\n",
    "    # List the properties of each CUDA device\n",
    "    for i in range(num_cuda_devices):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b13cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '8888' \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eefc49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-2-7b-chat/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#max_seq_len....\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/textconv/llama/llama/generation.py:119\u001b[0m, in \u001b[0;36mLlama.build\u001b[0;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)\u001b[0m\n\u001b[1;32m    117\u001b[0m model_args\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mn_words\n\u001b[1;32m    118\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_tensor_type(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mHalfTensor)\n\u001b[0;32m--> 119\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/projectnb/textconv/llama/llama/model.py:437\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mn_layers\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mParallelEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mn_layers):\n",
      "File \u001b[0;32m/projectnb/textconv/llama/packages/fairscale/nn/model_parallel/layers.py:190\u001b[0m, in \u001b[0;36mParallelEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, init_method, keep_master_weight_for_test)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim_per_partition \u001b[38;5;241m=\u001b[39m divide_and_check_no_remainder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim, world_size)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Allocate weights.\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim_per_partition\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# And initialize.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m _initialize_affine_weight(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     return_master_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(\n",
    "        ckpt_dir=\"llama-2-7b-chat/\",\n",
    "        tokenizer_path=\"tokenizer.model\",\n",
    "        max_seq_len=512, #max_seq_len....\n",
    "        max_batch_size=6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d608ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from NoiseKD\n",
    "import torch.nn as nn\n",
    "def Linearize_Embedding(embedding_layer):\n",
    "    embedding_weight_tensor = embedding_layer.weight.detach() \n",
    "    shape = embedding_weight_tensor.shape\n",
    "    vocab_size = shape[0]\n",
    "    embedding_dim = shape[1]\n",
    "    lin = nn.Linear(vocab_size,embedding_dim, bias = False)\n",
    "    #print(lin.weight.shape)\n",
    "    #print(embedding_weight_tensor.shape)\n",
    "    lin.weight = nn.Parameter(embedding_weight_tensor.T) #not sure about this transpose\n",
    "    return lin\n",
    "\n",
    "def batch_one_hot(input_sequences, vocab_size):\n",
    "    batch_size = input_sequences.size(0)\n",
    "    max_seq_length = input_sequences.size(1)\n",
    "    \n",
    "    # Create a tensor to store the one-hot encodings\n",
    "    one_hot_input = torch.zeros(batch_size, max_seq_length, vocab_size)\n",
    "    \n",
    "    # Use scatter_ to set the appropriate elements to 1 in each batch\n",
    "    one_hot_input.scatter_(2, input_sequences.unsqueeze(2), 1)\n",
    "    return one_hot_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d47048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_embeddings = Linearize_Embedding(generator.model.tok_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394f00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.tok_embeddings = float_embeddings #set, now it takes one hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d9c7753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_float_tensor(a = 0.0 \n",
    "                        ,b = 1.0\n",
    "                        ,max_len=512\n",
    "                       ,vocab_size =32_000 ):\n",
    "        # Replace with your desired lower bound\n",
    "         # Replace with your desired upper bound\n",
    "\n",
    "    random_int = random.randint(1, max_len)  #this is the random_input lenght\n",
    "    \n",
    "    # Generate the random tensor\n",
    "    random_tensor = torch.FloatTensor(1, random_int,vocab_size ).uniform_(a, b).to(torch.float16).to(\"cpu\")\n",
    "    #print(random_tensor.device)\n",
    "    zero_tensor = torch.zeros(1, max_len - random_int, vocab_size, dtype=torch.float16).to(\"cpu\") #this stays on the cpu for saving.\n",
    "    #print(zero_tensor.device)\n",
    "    cpu_tensor = torch.cat((random_tensor, zero_tensor), dim=1)\n",
    "    \n",
    "    return random_tensor,cpu_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effe1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  \"/projectnb/textconv/llama/tensor_dataset_2/\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "files_n = 5 #how many files to generate\n",
    "batch_per_file = 5 #how many batches in each file.  \n",
    "vocab_size = 32_000\n",
    "max_len = 512\n",
    "a = -1.0\n",
    "b = 2.0\n",
    "for i in range(files_n):\n",
    "    input_tensor = torch.zeros(batch_per_file\n",
    "                               ,max_len\n",
    "                               ,vocab_size\n",
    "                               ,dtype=torch.float16)\n",
    "    target_tensor = torch.zeros(batch_per_file\n",
    "                                ,vocab_size\n",
    "                               ,dtype=torch.float32)\n",
    "    for j in range(batch_per_file):\n",
    "        pred_tensor,save_tensor = random_float_tensor(max_len=512\n",
    "                       ,vocab_size =32_000)\n",
    "        input_tensor[j] = save_tensor\n",
    "        pred_tensor = pred_tensor.to(device)\n",
    "        model_output = generator.model.forward(pred_tensor,0)\n",
    "        target_tensor[j] = model_output[:,-1,:]\n",
    "    #print(f\"input{i}.pt\")\n",
    "    #print(input_tensor.shape,target_tensor.shape)\n",
    "    #print(f\"target{i}.pt\")\n",
    "    ##save\n",
    "    input_path = f\"{data_dir}input{i}.pt\"\n",
    "    target_path = f\"{data_dir}target{i}.pt\"\n",
    "    torch.save(input_tensor, input_path)\n",
    "    torch.save(target_tensor, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a446d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this a python file, and run it from cmd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "258e2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching files found in the directory.\n"
     ]
    }
   ],
   "source": [
    "data_dir =  \"/projectnb/textconv/llama/kd_data/\"\n",
    "max_number = -1\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.startswith(\"input\") and filename.endswith(\".pt\"):\n",
    "        try:\n",
    "            number = int(filename[len(\"input\"):-len(\".pt\")])\n",
    "            max_number = max(max_number, number)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "if max_number != -1:\n",
    "    print(f\"The maximum file number found is: {max_number}\")\n",
    "else:\n",
    "    print(\"No matching files found in the directory.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3445b3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_number + 1, max_number + 11):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4955042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
