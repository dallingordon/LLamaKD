{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82606b0a",
   "metadata": {},
   "source": [
    "this is for the positional embedding dev, primes and binary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f5e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "def int_to_binary_tensor(number, max_length):\n",
    "\n",
    "    binary_string = bin(number)[2:]  # Convert to binary and remove the '0b' prefix.\n",
    "    same_len = binary_string.zfill(max_length)\n",
    "    seperated =torch.tensor([float(i) for i in same_len])\n",
    "    return seperated\n",
    "def create_binary_tensor(input_length):\n",
    "    max_length = math.ceil(math.log2(input_length + 1))\n",
    "    binary_tensors = []\n",
    "\n",
    "    # Iterate through numbers from 1 to input_length\n",
    "    for number in range(1, input_length + 1):\n",
    "        binary_tensor = int_to_binary_tensor(number, max_length)\n",
    "        #print(binary_tensor.shape)\n",
    "        binary_tensors.append(binary_tensor)\n",
    "        \n",
    "    # Stack the binary tensors to form a 2D tensor\n",
    "    stacked_tensor = torch.stack(binary_tensors)\n",
    "\n",
    "    return stacked_tensor\n",
    "\n",
    "def binary_self_interactions(input_length):\n",
    "    stacked_tensor = create_binary_tensor(input_length)\n",
    "    res = torch.zeros((stacked_tensor.shape[0],stacked_tensor.shape[1]**2))\n",
    "    \n",
    "    for i in range(stacked_tensor.shape[0]):\n",
    "        res[i, :] = torch.outer(stacked_tensor[i],stacked_tensor[i]).flatten()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2530d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 10])\n",
      "torch.Size([512, 100])\n"
     ]
    }
   ],
   "source": [
    "e = create_binary_tensor(input_length = 512)\n",
    "print(e.shape)\n",
    "b = binary_self_interactions(input_length = 512)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a9dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce93c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BinaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(BinaryPositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.positional_input = binary_self_interactions(max_len)\n",
    "        \n",
    "        # Determine positional_emb_dim from positional_input\n",
    "        positional_emb_dim = self.positional_input.shape[1]\n",
    "        \n",
    "        # Linear layer with input size of positional_emb_dim and output size of embedding_dim\n",
    "        self.linear = nn.Linear(positional_emb_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Apply the linear layer to each position\n",
    "        x_add = self.linear(self.positional_input)\n",
    "        x_add = x_add.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        \n",
    "        return x + x_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "766a43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BinaryPositionalEmbedding(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5f216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.rand(2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf3e1cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bde272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
