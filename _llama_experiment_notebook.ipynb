{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e224c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source scc_scripts/scc_setup.sh not sure i need this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install webdataset -t /projectnb/textconv/llama/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882353ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/projectnb/textconv/llama/packages')\n",
    "\n",
    "import fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd9c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96eefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama, Dialog\n",
    "from llama.model import ModelArgs, Transformer\n",
    "from llama.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0e4f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 1\n",
      "Device 0: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_cuda_devices}\")\n",
    "\n",
    "    # List the properties of each CUDA device\n",
    "    for i in range(num_cuda_devices):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a39fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '8888' \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "#os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06950aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!echo $TORCH_USE_CUDA_DSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b516f837",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Loading a checkpoint for MP=1 but world size is None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-2-7b-chat/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#max_seq_len....\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/textconv/llama/llama/generation.py:103\u001b[0m, in \u001b[0;36mLlama.build\u001b[0;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)\u001b[0m\n\u001b[1;32m    101\u001b[0m checkpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(Path(ckpt_dir)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(checkpoints) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno checkpoint files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model_parallel_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    104\u001b[0m     checkpoints\n\u001b[1;32m    105\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a checkpoint for MP=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(checkpoints)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but world size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_parallel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m checkpoints[get_model_parallel_rank()]\n\u001b[1;32m    107\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Loading a checkpoint for MP=1 but world size is None"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(\n",
    "        ckpt_dir=\"llama-2-7b-chat/\",\n",
    "        tokenizer_path=\"tokenizer.model\",\n",
    "        max_seq_len=512, #max_seq_len....\n",
    "        max_batch_size=6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.tok_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a359ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##surgery\n",
    "batch_size = 1  # Replace with your desired batch size\n",
    "tensor_shape = (batch_size, 512)\n",
    "\n",
    "# Generate random integers in the tensor\n",
    "min_value = 0  # Minimum integer value (adjust as needed)\n",
    "max_value = 1000  # Maximum integer value (adjust as needed)\n",
    "random_tensor = torch.randint(min_value, max_value + 1, size=tensor_shape)\n",
    "#random_tensor\n",
    "emb_out = generator.model.tok_embeddings(random_tensor) #oh shit it worked.  should be input_len x embedding dim, so, 512 by 4096\n",
    "emb_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a298915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from NoiseKD\n",
    "import torch.nn as nn\n",
    "def Linearize_Embedding(embedding_layer):\n",
    "    embedding_weight_tensor = embedding_layer.weight.detach() \n",
    "    shape = embedding_weight_tensor.shape\n",
    "    vocab_size = shape[0]\n",
    "    embedding_dim = shape[1]\n",
    "    lin = nn.Linear(vocab_size,embedding_dim, bias = False)\n",
    "    #print(lin.weight.shape)\n",
    "    #print(embedding_weight_tensor.shape)\n",
    "    lin.weight = nn.Parameter(embedding_weight_tensor.T) #not sure about this transpose\n",
    "    return lin\n",
    "\n",
    "def batch_one_hot(input_sequences, vocab_size):\n",
    "    batch_size = input_sequences.size(0)\n",
    "    max_seq_length = input_sequences.size(1)\n",
    "    \n",
    "    # Create a tensor to store the one-hot encodings\n",
    "    one_hot_input = torch.zeros(batch_size, max_seq_length, vocab_size)\n",
    "    \n",
    "    # Use scatter_ to set the appropriate elements to 1 in each batch\n",
    "    one_hot_input.scatter_(2, input_sequences.unsqueeze(2), 1)\n",
    "    return one_hot_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6539370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_embeddings = Linearize_Embedding(generator.model.tok_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7310c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.tok_embeddings = float_embeddings #set, now it takes one hots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b012773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.randint(min_value, max_value + 1, size=tensor_shape)\n",
    "oh_random_tensor = batch_one_hot(random_tensor, 32_000)\n",
    "oh_random_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f72ff047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 32000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = generator.model.forward(oh_random_tensor,0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e42433",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_random_tensor[:,:20,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8846de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_emb_out = float_embeddings(oh_random_tensor[:,:20,:]) #oh duh, onehottify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_emb_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa05cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def random_float_tensor(a = 0.0 \n",
    "                        ,b = 1.0\n",
    "                        ,max_len=512\n",
    "                       ,vocab_size =32_000 ):\n",
    "        # Replace with your desired lower bound\n",
    "         # Replace with your desired upper bound\n",
    "\n",
    "    random_int = random.randint(1, max_len)  #this is the random_input lenght\n",
    "    \n",
    "    # Generate the random tensor\n",
    "    random_tensor = torch.FloatTensor(1, random_int,vocab_size ).uniform_(a, b).to(torch.float16).to(\"cpu\")\n",
    "    #print(random_tensor.device)\n",
    "    zero_tensor = torch.zeros(1, max_len - random_int, vocab_size, dtype=torch.float16).to(\"cpu\") #this stays on the cpu for saving.\n",
    "    #print(zero_tensor.device)\n",
    "    cpu_tensor = torch.cat((random_tensor, zero_tensor), dim=1)\n",
    "    \n",
    "    return random_tensor,cpu_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb34ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor,b = random_float_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "913ebe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 289, 32000]) tensor([[[-0.4016,  4.3711,  0.4324,  ..., -1.7080,  0.2542, -2.3613],\n",
      "         [-0.4260,  4.3672,  0.5054,  ..., -1.7080,  0.2800, -2.3516],\n",
      "         [-0.4121,  4.3086,  0.5000,  ..., -1.7832,  0.1028, -2.4062],\n",
      "         ...,\n",
      "         [-0.4094,  4.4219,  0.4758,  ..., -1.7314,  0.2961, -2.4434],\n",
      "         [-0.4019,  4.2852,  0.5273,  ..., -1.7930,  0.2795, -2.3223],\n",
      "         [-0.4092,  4.3672,  0.4580,  ..., -1.7949,  0.2830, -2.4043]]],\n",
      "       dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "x = generator.model.forward(random_tensor.to(device),0) #it overrides forward in model.py.  okay.  more work...\n",
    "#https://github.com/facebookresearch/llama/blob/main/llama/model.py#L469 transformer forward...\n",
    "#gorge\n",
    "print(x.shape, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "59c26647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 289, 32000]), torch.Size([1, 289, 32000]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,random_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4aecac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "960f70c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 32000]), torch.Size([1, 174, 32000]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, random_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1f5ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "####generates and saves data\n",
    "#####make this data loader next\n",
    "data_dir =  \"/projectnb/textconv/llama/tensor_dataset/\"\n",
    "files_n = 3 #how many files to generate\n",
    "batch_per_file = 2 #how many batches in each file.  \n",
    "vocab_size = 32_000\n",
    "max_len = 512\n",
    "for i in range(files_n):\n",
    "    input_tensor = torch.zeros(batch_per_file\n",
    "                               ,max_len\n",
    "                               ,vocab_size\n",
    "                               ,dtype=torch.float16)\n",
    "    target_tensor = torch.zeros(batch_per_file\n",
    "                                ,vocab_size\n",
    "                               ,dtype=torch.float32)\n",
    "    for j in range(batch_per_file):\n",
    "        pred_tensor,save_tensor = random_float_tensor(max_len=512\n",
    "                       ,vocab_size =32_000)\n",
    "        input_tensor[j] = save_tensor\n",
    "        pred_tensor = pred_tensor.to(device)\n",
    "        model_output = generator.model.forward(pred_tensor,0)\n",
    "        target_tensor[j] = model_output[:,-1,:]\n",
    "    #print(f\"input{i}.pt\")\n",
    "    #print(input_tensor.shape,target_tensor.shape)\n",
    "    #print(f\"target{i}.pt\")\n",
    "    ##save\n",
    "    input_path = f\"{data_dir}input{i}.pt\"\n",
    "    target_path = f\"{data_dir}target{i}.pt\"\n",
    "    torch.save(input_tensor, input_path)\n",
    "    torch.save(target_tensor, target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df88dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32000])\n"
     ]
    }
   ],
   "source": [
    "file_path = data_dir + 'target2.pt'\n",
    "\n",
    "# Load the tensor from the specified file path\n",
    "loaded_tensor = torch.load(file_path)\n",
    "print(loaded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db07b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_batches, batch_size):\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate some sample data for demonstration purposes\n",
    "        batch_data = torch.randn(self.batch_size, 1)  # Replace with your actual data loading logic\n",
    "        return batch_data\n",
    "\n",
    "# Number of batches and batch size\n",
    "num_batches = 1000\n",
    "batch_size = 100\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomDataset(num_batches, batch_size)\n",
    "\n",
    "# Create a BatchSampler to shuffle batches\n",
    "batch_sampler = BatchSampler(\n",
    "    sampler=torch.randperm(len(custom_dataset)),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(custom_dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "# Iterate over shuffled batches\n",
    "for batch in dataloader:\n",
    "    # Your training code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###messing with wds\n",
    "data_dir =  \"/projectnb/textconv/llama/kd_data/test_kd/\"\n",
    "dir_num = 4 #number of shards i guess you could say, \n",
    "\n",
    "# Create directories if they don't exist and generate and save data\n",
    "for i in range(dir_num):\n",
    "    # Create a directory name\n",
    "    dir_name = os.path.join(data_dir, f\"sample{i}\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    \n",
    "    #generate data from lama and random noise\n",
    "    #input_data, label_data = gen_data()\n",
    "    ######## There are globals here.  this needs to be fixed.  \n",
    "    random_tensor = torch.randint(min_value, max_value + 1, size=tensor_shape)\n",
    "    oh_random_tensor = batch_one_hot(random_tensor, 32_000)\n",
    "    label_data = generator.model.forward(oh_random_tensor,0)\n",
    "    \n",
    "    input_data = oh_random_tensor\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save the input data and label data as PyTorch tensors\n",
    "    input_file_path = os.path.join(dir_name, \"1.data.pt\")\n",
    "    label_file_path = os.path.join(dir_name, \"1.labels.pt\")\n",
    "    \n",
    "    torch.save(input_data, input_file_path)\n",
    "    torch.save(label_data, label_file_path)\n",
    "#this made the pt files.  looks good.  making generator now...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3560c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cf test_kd_dataset.tar /projectnb/textconv/llama/kd_data/test_kd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0178c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "# Define a dataset loader\n",
    "dataset = wds.WebDataset(\"/projectnb/textconv/llama/test_kd_dataset.tar\")\n",
    "\n",
    "# Iterate through the dataset samples\n",
    "for data in dataset:\n",
    "    print(data.keys())\n",
    "#talk to christopher.  not sure this is the way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls kd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec75b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##oh_random_tensor.shape[:2], random_tensor.shape[:2] #this might be the way. worked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ffd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_out.shape, f_emb_out.shape #shapes match\n",
    "torch.all(emb_out == f_emb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.output.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(generator.tokenizer) ##here she is...\n",
    "#https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L159 look here.  its setting up the input.  \n",
    "# i think i'll end up editing that, swapping out llama and, either swapping out tokenizer or fixing the functions for the reduced vocab size. idk yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(generator.tokenizer.sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e98bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.tokenizer.sp_model.vocab_size() #ahaaaa.  its vocab size.   fuck yeah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.tokenizer.sp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for piece_id in range(100):\n",
    "    piece = generator.tokenizer.sp_model.id_to_piece(piece_id)\n",
    "    vocabulary[piece] = piece_id\n",
    "\n",
    "# Now you have a dictionary mapping pieces to their IDs\n",
    "print(vocabulary) #this is term to id.\n",
    "#sick, so, limit to certain ids i think? hahahaha i'm coming for you zuckerberg lololol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e57604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.tokenizer.sp_model.id_to_piece(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "###https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L189\n",
    "# the model doesn't have soft max at the end.  yeah.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path \"tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b3743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "dialogs: List[Dialog] = [\n",
    "        [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\"\"\\\n",
    "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
    "\n",
    "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
    "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
    "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
    "\n",
    "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Always answer with emojis\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Write a brief birthday message to John\"},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Unsafe [/INST] prompt using [INST] special tags\",\n",
    "            }\n",
    "        ],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e770a278",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2069120592.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dialogs: List[Dialog],\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dialogs: List[Dialog],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d322d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = generator.chat_completion( dialogs=dialogs,  ##calls generate at line 364 https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L364\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        max_gen_len=512,\n",
    "        logprobs= True,) #see what this does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32784598",
   "metadata": {},
   "outputs": [],
   "source": [
    "res #making sure my indexing didn't break anything gorge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[0].keys()) #each item in the batch is in the res list, then its a dict.\n",
    "#len(res[5]['logprobs']) #why does it change....is it one per token maybe?\n",
    "res[5]['tokens'] #yup, one logit per token.  gonna write code from the project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs[1] ## okay, so you can give it all that.  what this look like tokenized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a143d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "##in generation.py https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L342\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\" #beginning end, here: https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L44\n",
    "prompt_tokens = generator.tokenizer.encode(\n",
    "                        f\"hi there would you please\",\n",
    "                        bos=True,\n",
    "                        eos=True,\n",
    "                    )\n",
    "type(prompt_tokens),len(prompt_tokens), prompt_tokens #list of ints.  gets made for the model later....\n",
    "#https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L365 prompt tokens is passed to generate.  \n",
    "#https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L159 bsz=len(prompt_tokens)  we about to do math...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = generator.tokenizer.pad_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7325e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.tokenizer.pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ce33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = generator.model.params.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a25ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = [prompt_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_len = 512 #min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "bsz = len(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\") ##somehitng with the padid.  -1 is out of bounts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume here https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L169\n",
    "\n",
    "for k, t in enumerate(prompt_tokens):\n",
    "    tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "    #if logprobs:\n",
    "    #    token_logprobs = torch.zeros_like(tokens, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f605558",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_mask = tokens != pad_id\n",
    "input_text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hahahhaah.   here it is babay.  \n",
    "model_result = generator.model.forward(tokens[:,0:5],0)\n",
    "#should be size of the vocab...\n",
    "model_result.shape,model_result #hahahahaha.  got it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d89e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6  # Replace with your desired batch size\n",
    "tensor_shape = (batch_size, 512)\n",
    "\n",
    "# Generate random integers in the tensor\n",
    "min_value = 0  # Minimum integer value (adjust as needed)\n",
    "max_value = 1000  # Maximum integer value (adjust as needed)\n",
    "random_tensor = torch.randint(min_value, max_value + 1, size=tensor_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_tensor.shape\n",
    "rand_res = generator.model.forward(random_tensor,0) #ran!\n",
    "rand_res.shape,rand_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model.tok_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e923a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lets see what the pad-id really is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45136997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "echo \"Loading Modules\"\n",
    "module load python3/3.10.5\n",
    "# pip install torch==2.1.0 --target /projectnb/textconv/dgordon/packages --upgrade\n",
    "module load gcc/8.3.0\n",
    "module load cuda/12.2 # trying this, kernel uses 12.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
