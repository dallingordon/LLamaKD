{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc4ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1724762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossBaby_1(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(CrossBaby_1, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.out = nn.Linear(self.word_embed,self.vocab_size)\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        print(x.shape, \"after reduce\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        print(x.shape, \"after reshape 2\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        print(x.shape,\"after reduce_2\")\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class CrossBaby_2(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , up_dim\n",
    "                ):\n",
    "        super(CrossBaby_2, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        \n",
    "        self.x_a_linear = nn.Linear(self.word_embed*self.sequence_length,self.word_embed)\n",
    "        self.a_down = nn.Linear(self.word_embed*self.sequence_length,self.word_embed )\n",
    "        \n",
    "        self.x_b_linear = nn.Linear(self.word_embed*self.sequence_length,self.word_embed)\n",
    "        self.b_down = nn.Linear(self.word_embed*self.sequence_length,self.word_embed )\n",
    "        \n",
    "        self.x_c_linear = nn.Linear(self.sequence_length*self.sequence_length,self.word_embed)\n",
    "        self.c_down = nn.Linear(self.word_embed*self.word_embed,self.word_embed )\n",
    "        \n",
    "        self.out_1 = nn.Linear(3*self.word_embed,up_dim)\n",
    "        self.out_2 = nn.Linear(up_dim,vocab_size )\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x_a = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x_a.shape, \"x_a\")\n",
    "        a = F.relu(self.x_a_linear(x_a))\n",
    "        a = a.reshape(a.shape[0],-1)\n",
    "        a = F.relu(self.a_down(a))\n",
    "        #print(a.shape,\"a\")\n",
    "        x_b = x.permute(0,2,1,3).reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x_b.shape, \"x_b\")\n",
    "        b = F.relu(self.x_b_linear(x_b))\n",
    "        b = b.reshape(b.shape[0],-1)\n",
    "        b = F.relu(self.b_down(b))\n",
    "        #print(b.shape, \"b\")\n",
    "        x_c = x.permute(0,3,1,2).reshape(x.shape[0],x.shape[-1],-1)\n",
    "        #print(x_c.shape, \"x_c\")\n",
    "        c = F.relu(self.x_c_linear(x_c))\n",
    "        c = c.reshape(c.shape[0],-1)\n",
    "        c = F.relu(self.c_down(c))\n",
    "        #print(c.shape)\n",
    "        d = torch.concat((a,b,c), dim=-1)\n",
    "        #print(d.shape)\n",
    "        d = F.relu(self.out_1(d))\n",
    "        d = self.out_2(d)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ce63327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 36060050\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "    , \"up_dim\":500\n",
    "}\n",
    "\n",
    "model = CrossBaby_2(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1999d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to CrossBaby_2_50.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"CrossBaby_2_50.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f84b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TransformerBaby(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer model that accepts one-hot encoded input and outputs a tensor of shape batch x vocab_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size\n",
    "                 , sequence_length\n",
    "                 , d_model\n",
    "                 , nhead\n",
    "                 , num_layers\n",
    "                 , dim_feedforward\n",
    "                ):\n",
    "        super(TransformerBaby, self).__init__()\n",
    "\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Linear layer to match d_model size\n",
    "        self.word_embedding = nn.Linear(vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, sequence_length)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.fc = nn.Linear(d_model * sequence_length, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.word_embedding(src)\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        output = self.transformer_encoder(src)\n",
    "        print(output.shape, \"pre-flat\")\n",
    "        output = output.view(output.size(0), -1)  # Flattening\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding that adds position information to input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35576aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to transformer_1.json\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "                 , \"sequence_length\": 512\n",
    "                 , \"d_model\":64\n",
    "                 , \"nhead\": 2\n",
    "                 , \"num_layers\": 2\n",
    "                 , \"dim_feedforward\":32\n",
    "}\n",
    "\n",
    "model = TransformerBaby(**kwargs)\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"transformer_1.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfca9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 64]) pre-flat\n",
      "torch.Size([2, 512, 32000]) torch.Size([2, 32000])\n",
      "Total parameters in DimMemory layer: 1050698240\n"
     ]
    }
   ],
   "source": [
    "i = torch.randn((2,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb3a26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBaby(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim #vestigial lololol\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                ):\n",
    "        super(LlamaBaby, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim) #could apply twice\n",
    "        self.out_down = nn.Linear(self.sentence_embed,self.balanced_dim)\n",
    "        self.out = nn.Linear(self.balanced_dim**3,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = F.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = F.relu(self.out_down(x)) \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class MemoryBaby(nn.Module):\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                 , mem_input_dim\n",
    "                 , mem_hidden_dim\n",
    "                 , mem_output_dim\n",
    "                 , memory_dim\n",
    "                ):\n",
    "        super(MemoryBaby, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim)\n",
    "        self.out_down = nn.Linear(self.sentence_embed,self.balanced_dim)\n",
    "        self.to_mem = nn.Linear(self.balanced_dim**3,mem_input_dim)\n",
    "        self.dim_memory = DimMemory(mem_input_dim,mem_hidden_dim,mem_output_dim,memory_dim)\n",
    "        \n",
    "        self.out = nn.Linear(mem_input_dim + mem_output_dim,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Flatten the sequence_length and vocab_size dimensions\n",
    "        x = torch.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = torch.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = torch.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = torch.relu(self.out_down(x)) #duplicated, could be a second layer.   \n",
    "        \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = torch.relu(self.to_mem(x))\n",
    "        y = self.dim_memory(x)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fce7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"sequence_length\": 512,\n",
    "    \"hidden_dim\": 11,\n",
    "    \"word_embed\": 12,\n",
    "    \"sentence_embed\": 13,\n",
    "    \"balanced_dim\": 14\n",
    "}\n",
    "model = LlamaBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d94b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"sequence_length\": 512,\n",
    "    \"hidden_dim\": 11,\n",
    "    \"word_embed\": 12,\n",
    "    \"sentence_embed\": 13,\n",
    "    \"balanced_dim\": 14\n",
    "}\n",
    "model = LlamaBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9d68242",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'vocab_size': 32_000\n",
    "     , 'sequence_length': 512\n",
    "     , 'hidden_dim': 10\n",
    "     , 'word_embed': 11\n",
    "     , 'sentence_embed': 12\n",
    "     , 'balanced_dim': 13\n",
    "     , 'mem_input_dim': 14 \n",
    "     , 'mem_hidden_dim': 15\n",
    "     , 'mem_output_dim': 16\n",
    "     , 'memory_dim': 4\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = MemoryBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac8208ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_baby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m32_000\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape, \u001b[43mllama_baby\u001b[49m(i)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal parameters in DimMemory layer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama_baby' is not defined"
     ]
    }
   ],
   "source": [
    "i = torch.randn((2,512,32_000))\n",
    "print(i.shape, llama_baby(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d7cf5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaFetus(nn.Module):\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim #vestigial lololol\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                ):\n",
    "        super(LlamaFetus, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim) #could apply twice\n",
    "    \n",
    "        self.out = nn.Linear(self.balanced_dim**3,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Flatten the sequence_length and vocab_size dimensions\n",
    "        x = F.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = F.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = F.relu(self.seq_down(x)) #duplicated, could be a second layer.   \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        # Pass through the second fully connected layer\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "class DimMemory(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, memory_dim):\n",
    "        super(DimMemory, self).__init__()\n",
    "        if memory_dim < 1:\n",
    "            raise ValueError(\"memory_dim must be greater than or equal to 1.\")\n",
    "        \n",
    "        self.memory_dim = memory_dim \n",
    "        # Create memory tensor with dynamic shape\n",
    "        mem_args = [1]\n",
    "        mem_args.extend([hidden_dim for _ in range(memory_dim)])\n",
    "        self.mem = nn.Parameter(torch.randn(*mem_args))\n",
    "        self.mem.requires_grad = True\n",
    "        \n",
    "        # Create a list of linear layers with memory_dim - 1 repetitions\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(input_dim, hidden_dim) for _ in range(memory_dim - 1)\n",
    "        ])\n",
    "\n",
    "        # Create the final linear layer for output\n",
    "        self.linear_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        # Expand memory tensor along the third dimension\n",
    "        mem_forward_args = [input1.shape[0]]\n",
    "        mem_forward_args.extend([-1 for _ in range(self.memory_dim)])\n",
    "        \n",
    "        x = self.mem.expand(*mem_forward_args)\n",
    "        \n",
    "        for i, linear_layer in enumerate(self.linears):\n",
    "            #print(linear_layer.weight.shape)\n",
    "            \n",
    "            y = torch.relu(linear_layer(input1))\n",
    "            print(y.shape, x.shape)\n",
    "            x = torch.einsum('az,a...yz->a...y',y,x)\n",
    "            #print(x.shape)\n",
    "        # Apply the final linear layer for output\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.linear_out(x))\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class MemoryLlama(nn.Module):\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                 , mem_input_dim\n",
    "                 , mem_hidden_dim\n",
    "                 , mem_output_dim\n",
    "                 , memory_dim\n",
    "                ):\n",
    "        super(MemoryLlama, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim) #could apply twice\n",
    "        self.to_mem = nn.Linear(self.balanced_dim**3,mem_input_dim)\n",
    "        self.dim_memory = DimMemory(mem_input_dim,mem_hidden_dim,mem_output_dim,memory_dim)\n",
    "        \n",
    "        self.out = nn.Linear(mem_input_dim + mem_output_dim,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Flatten the sequence_length and vocab_size dimensions\n",
    "        x = torch.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = torch.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = torch.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = torch.relu(self.seq_down(x)) #duplicated, could be a second layer.   \n",
    "        \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = torch.relu(self.to_mem(x))\n",
    "        y = self.dim_memory(x)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "397a0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'vocab_size': 32_000\n",
    "     , 'sequence_length': 512\n",
    "     , 'hidden_dim': 100\n",
    "     , 'word_embed': 50\n",
    "     , 'sentence_embed': 200\n",
    "     , 'balanced_dim': 40\n",
    "     , 'mem_input_dim': 20 \n",
    "     , 'mem_hidden_dim': 40\n",
    "     , 'mem_output_dim': 100\n",
    "     , 'memory_dim': 4\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = MemoryBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "006b6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40]) torch.Size([1, 40, 40, 40, 40])\n",
      "torch.Size([1, 40]) torch.Size([1, 40, 40, 40])\n",
      "torch.Size([1, 40]) torch.Size([1, 40, 40])\n",
      "torch.Size([1, 120])\n",
      "torch.Size([1, 512, 32000]) torch.Size([1, 32000])\n",
      "Total parameters in DimMemory layer: 9439410\n"
     ]
    }
   ],
   "source": [
    "i = torch.randn((1,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d595a654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9439410"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1efe7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to membaby_small_9mil.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"membaby_small_9mil.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_layer = DimMemory(input_dim = 200,hidden_dim=11, output_dim=20,memory_dim=6)\n",
    "\n",
    "# Assuming you have an input tensor input1 with shape (batch_size, 10)\n",
    "input1 = torch.ones(4,200)\n",
    "out2 = memory_layer(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in memory_layer.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    memory_layer = DimMemory(input_dim = 20,hidden_dim=10, output_dim=20,memory_dim=i)\n",
    "    input1 = torch.ones(4,20)\n",
    "    out2 = memory_layer(input1)\n",
    "    total_params = sum(p.numel() for p in memory_layer.parameters())\n",
    "    formatted_total_params = '{:,}'.format(total_params)\n",
    "    print(\"Memory_dim=\",i,\"Total parameters in DimMemory layer:\", formatted_total_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89120f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"sequence_length\": 512,\n",
    "    \"hidden_dim\": None,\n",
    "    \"word_embed\": 100,\n",
    "    \"sentence_embed\": 1000,\n",
    "    \"balanced_dim\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8d3e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in DimMemory layer: 259767140\n"
     ]
    }
   ],
   "source": [
    "model = LlamaFetus(**kwargs)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706895be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimMemory(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, memory_dim):\n",
    "        super(DimMemory, self).__init__()\n",
    "        if memory_dim < 1:\n",
    "            raise ValueError(\"memory_dim must be greater than or equal to 1.\")\n",
    "        \n",
    "        self.memory_dim = memory_dim \n",
    "        # Create memory tensor with dynamic shape\n",
    "        mem_args = [1]\n",
    "        mem_args.extend([hidden_dim for _ in range(memory_dim)])\n",
    "        self.mem = nn.Parameter(torch.randn(*mem_args))\n",
    "        self.mem.requires_grad = True\n",
    "        \n",
    "        # Create a list of linear layers with memory_dim - 1 repetitions\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(input_dim, hidden_dim) for _ in range(memory_dim - 1)\n",
    "        ])\n",
    "\n",
    "        # Create the final linear layer for output\n",
    "        self.linear_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        # Expand memory tensor along the third dimension\n",
    "        mem_forward_args = [input1.shape[0]]\n",
    "        mem_forward_args.extend([-1 for _ in range(self.memory_dim)])\n",
    "        \n",
    "        x = self.mem.expand(*mem_forward_args)\n",
    "        \n",
    "        for i, linear_layer in enumerate(self.linears):\n",
    "            #print(linear_layer.weight.shape)\n",
    "            \n",
    "            y = torch.relu(linear_layer(input1))\n",
    "            print(y.shape, x.shape)\n",
    "            x = torch.einsum('az,a...yz->a...y',y,x)\n",
    "            #print(x.shape)\n",
    "        # Apply the final linear layer for output\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.linear_out(x))\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9189011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareMemory(nn.Module):\n",
    "    def __init__(self,input_dim, memory_dim):\n",
    "        super(SquareMemory, self).__init__()\n",
    "        \n",
    "        self.mem = nn.Parameter(torch.randn(memory_dim,memory_dim))\n",
    "        self.mem.requires_grad = True\n",
    "        \n",
    "        \n",
    "\n",
    "        # Create the final linear layer for output\n",
    "        self.first_axis = nn.Linear(input_dim, memory_dim)\n",
    "        self.second_axis = nn.Linear(input_dim, memory_dim)\n",
    "        self.intercept = nn.Linear(input_dim,memory_dim)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        #batch_dim = input1.shape[0]\n",
    "        #print(batch_dim)\n",
    "        first = F.softmax(self.first_axis(input1), dim=-1)\n",
    "        first = torch.matmul(first, self.mem) #first is used as the lookup, picks a row with the softmax\n",
    "        \n",
    "        second = F.relu(self.second_axis(input1))\n",
    "        \n",
    "        x = second * first\n",
    "        \n",
    "        intercept = F.relu(self.intercept(input1))\n",
    "        x = x + intercept\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SMCrossBaby_1(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , memory_dim\n",
    "                ):\n",
    "        super(SMCrossBaby_1, self).__init__()\n",
    "        \n",
    "        self.mem = SquareMemory(word_embed,memory_dim)\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.out = nn.Linear(memory_dim,self.vocab_size)\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        #print(x.shape, \"after reduce\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        #print(x.shape, \"after reshape 2\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        #print(x.shape,\"after reduce_2\")\n",
    "        x = self.mem(x)\n",
    "        #print(x.shape, \"memory\")\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class SMCrossBaby_Concat(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , memory_dim\n",
    "                ):\n",
    "        super(SMCrossBaby_Concat, self).__init__()\n",
    "        \n",
    "        self.mem = SquareMemory(word_embed,memory_dim)\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.conc = nn.Linear(self.sequence_length*self.word_embed,memory_dim)\n",
    "        \n",
    "        \n",
    "        self.out = nn.Linear(memory_dim*2,self.vocab_size)\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        #print(x.shape, \"after reduce\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        #print(x.shape, \"after reshape 2\")\n",
    "        c = F.relu(self.conc(x))\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        #print(x.shape,\"after reduce_2\")\n",
    "        x = self.mem(x)\n",
    "        x = torch.concat((x, c), dim = -1)\n",
    "        #print(x.shape, 'after concat')\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class SMCrossBaby_2(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , up_dim\n",
    "                ):\n",
    "        super(SMCrossBaby_2, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        \n",
    "        self.x_a_linear = nn.Linear(self.word_embed*self.sequence_length,self.word_embed)\n",
    "        self.a_down = nn.Linear(self.word_embed*self.sequence_length,self.word_embed )\n",
    "        \n",
    "        self.x_b_linear = nn.Linear(self.word_embed*self.sequence_length,self.word_embed)\n",
    "        self.b_down = nn.Linear(self.word_embed*self.sequence_length,self.word_embed )\n",
    "        \n",
    "        self.x_c_linear = nn.Linear(self.sequence_length*self.sequence_length,self.word_embed)\n",
    "        self.c_down = nn.Linear(self.word_embed*self.word_embed,self.word_embed )\n",
    "        \n",
    "        self.out_1 = nn.Linear(3*self.word_embed,up_dim)\n",
    "        self.out_2 = nn.Linear(up_dim,vocab_size )\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x_a = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x_a.shape, \"x_a\")\n",
    "        a = F.relu(self.x_a_linear(x_a))\n",
    "        a = a.reshape(a.shape[0],-1)\n",
    "        a = F.relu(self.a_down(a))\n",
    "        #print(a.shape,\"a\")\n",
    "        x_b = x.permute(0,2,1,3).reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x_b.shape, \"x_b\")\n",
    "        b = F.relu(self.x_b_linear(x_b))\n",
    "        b = b.reshape(b.shape[0],-1)\n",
    "        b = F.relu(self.b_down(b))\n",
    "        #print(b.shape, \"b\")\n",
    "        x_c = x.permute(0,3,1,2).reshape(x.shape[0],x.shape[-1],-1)\n",
    "        #print(x_c.shape, \"x_c\")\n",
    "        c = F.relu(self.x_c_linear(x_c))\n",
    "        c = c.reshape(c.shape[0],-1)\n",
    "        c = F.relu(self.c_down(c))\n",
    "        #print(c.shape)\n",
    "        d = torch.concat((a,b,c), dim=-1)\n",
    "        #print(d.shape)\n",
    "        d = F.relu(self.out_1(d))\n",
    "        \n",
    "        d = self.out_2(d)\n",
    "        return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aeebd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randn(5,512,32_000)\n",
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":100\n",
    "    , \"memory_dim\":500\n",
    "}\n",
    "model = SMCrossBaby_1(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "02d5b1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32000])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ece2c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512, 32000]) torch.Size([5, 32000])\n",
      "Total parameters in DimMemory layer: 29873800\n"
     ]
    }
   ],
   "source": [
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "faa613e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randn(5,512,32_000)\n",
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":100\n",
    "    , \"memory_dim\":1000\n",
    "}\n",
    "model = SMCrossBaby_Concat(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b5cdcac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512, 32000]) torch.Size([5, 32000])\n",
      "Total parameters in DimMemory layer: 129976300\n"
     ]
    }
   ],
   "source": [
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a23b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to SMCrossBaby_concat1k.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"SMCrossBaby_concat1k.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "93bdbaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 32000,\n",
       " 'sequence_length': 512,\n",
       " 'word_embed': 100,\n",
       " 'memory_dim': 1000}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033c4c9",
   "metadata": {},
   "source": [
    "couple things.  First, lower triangle, do we need it? i think that might save us lots of stuff.  no need to do the cross products twice\n",
    "\n",
    "next, crossbaby, mak the mem higher dim.  i like the softmax and final thing.  maybe it doesn't need to be a perfect hypercube? maybe only the last thing needs to be big.  like, \n",
    "\n",
    "20 to the 4th is how many things, 160,000, with an embedding dim of like, 200, 32,000,000 possible embeddings, and all those are floats.....i like that thought.  \n",
    "\n",
    "positional embedding.  do somthind that is 3 dimensional.  one for each prime, and a shared transform for the power? one is positional, one is the number, and they interact somehow? \n",
    "\n",
    "512 only has like 100 primes less than, try binary maybe? that is interesting, but not sure it gets the 3 divisibility.  look at it.  its in positional_embedding dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f09c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleCrossBaby(nn.Module):\n",
    "    \"\"\"2 interactions in the embedding space.  then sums.  that sum could be learned?\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(DoubleCrossBaby, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        \n",
    "        self.up_1 = nn.Linear(self.word_embed*5, self.word_embed*10)\n",
    "        self.up_2 = nn.Linear(self.word_embed*10,self.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        #print(x.shape, \"after reduce\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.up_1(x))\n",
    "        #print(x.shape, \"up_1\")\n",
    "        x = F.relu(self.up_2(x))\n",
    "        #print(x.shape,\"up_2\")\n",
    "        x = torch.sum(x, dim=1)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class LearnedDoubleCrossBaby(nn.Module):\n",
    "    \"\"\"2 interactions in the embedding space.  then sums.  that sum could be learned?\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(LearnedDoubleCrossBaby, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        \n",
    "        self.up_1 = nn.Linear(self.word_embed*5, self.word_embed*10)\n",
    "        self.up_2 = nn.Linear(self.word_embed*10,self.vocab_size)\n",
    "        self.final = nn.Linear(self.sequence_length, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        #print(x.shape, \"after reduce\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.up_1(x))\n",
    "        #print(x.shape, \"up_1\")\n",
    "        x = F.relu(self.up_2(x))\n",
    "        #print(x.shape,\"up_2\")\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.final(x))\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87f9e51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 500]) up_1\n",
      "torch.Size([10, 512, 32000]) up_2\n",
      "torch.Size([10, 32000])\n",
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 33118413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "}\n",
    "\n",
    "model = LearnedDoubleCrossBaby(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3af4729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to LearnedDoubleCrossBaby_conf.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"LearnedDoubleCrossBaby_conf.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40a5850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]),\n",
       " tensor([[0, 1, 2, 3],\n",
       "         [0, 1, 2, 4],\n",
       "         [4, 5, 6, 5]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0,1,2,3],[0,1,2,4],[4,5,6,5]])\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fac6b",
   "metadata": {},
   "source": [
    "ok, now i want to have the learned part of LearnedDoubleCrossBaby to be dependent on some time embedding.  i want to try my binary decomp.  use that, multiply with a tensor, and that result is multiplied by the final bit.  then project to output space.  \n",
    "\n",
    "adding a third cross product\n",
    "\n",
    "incorporating memory cell.   \n",
    "\n",
    "try em all.  I think with the learning scheme its more important to find a good architecture.  something that can do better than platesplatesplatesplates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d07ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def int_to_binary_tensor(number, max_length):\n",
    "\n",
    "    binary_string = bin(number)[2:]  # Convert to binary and remove the '0b' prefix.\n",
    "    same_len = binary_string.zfill(max_length)\n",
    "    seperated =torch.tensor([float(i) for i in same_len])\n",
    "    return seperated\n",
    "def create_binary_tensor(input_length):\n",
    "    max_length = math.ceil(math.log2(input_length + 1))\n",
    "    binary_tensors = []\n",
    "\n",
    "    # Iterate through numbers from 1 to input_length\n",
    "    for number in range(1, input_length + 1):\n",
    "        binary_tensor = int_to_binary_tensor(number, max_length)\n",
    "        #print(binary_tensor.shape)\n",
    "        binary_tensors.append(binary_tensor)\n",
    "        \n",
    "    # Stack the binary tensors to form a 2D tensor\n",
    "    stacked_tensor = torch.stack(binary_tensors)\n",
    "\n",
    "    return stacked_tensor\n",
    "\n",
    "def binary_self_interactions(input_length):\n",
    "    stacked_tensor = create_binary_tensor(input_length)\n",
    "    res = torch.zeros((stacked_tensor.shape[0],stacked_tensor.shape[1]**2))\n",
    "    \n",
    "    for i in range(stacked_tensor.shape[0]):\n",
    "        res[i, :] = torch.outer(stacked_tensor[i],stacked_tensor[i]).flatten()\n",
    "    return res\n",
    "\n",
    "class BinaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(BinaryPositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.positional_input = binary_self_interactions(max_len)\n",
    "        \n",
    "        # Determine positional_emb_dim from positional_input\n",
    "        positional_emb_dim = self.positional_input.shape[1]\n",
    "        \n",
    "        # Linear layer with input size of positional_emb_dim and output size of embedding_dim\n",
    "        self.linear = nn.Linear(positional_emb_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Apply the linear layer to each position\n",
    "        x_add = self.linear(self.positional_input)\n",
    "        x_add = x_add.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        \n",
    "        return x + x_add\n",
    "\n",
    "\n",
    "class LearnedDoubleCrossBabyWithBinaryEmbedding(nn.Module):\n",
    "    \"\"\"2 interactions in the embedding space.  then sums.  that sum could be learned?\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(LearnedDoubleCrossBabyWithBinaryEmbedding, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.word_pos_emb = BinaryPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = BinaryPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = BinaryPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.up_1 = nn.Linear(self.word_embed*5, self.word_embed*10)\n",
    "        self.up_2 = nn.Linear(self.word_embed*10,self.vocab_size)\n",
    "        self.final = nn.Linear(self.sequence_length, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        #print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        #print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.up_1(x))\n",
    "        #print(x.shape, \"up_1\")\n",
    "        x = F.relu(self.up_2(x))\n",
    "        #print(x.shape,\"up_2\")\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.final(x))\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733298c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 33158813\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "}\n",
    "\n",
    "model = LearnedDoubleCrossBabyWithBinaryEmbedding(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72893773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to LearnedDoubleCrossBaby_withbinaryemb_conf.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"LearnedDoubleCrossBaby_withbinaryemb_conf.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f10afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do i want to add positional embeddings to other models? that isn't a bad idea.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ef0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing sine learned and not:\n",
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = torch.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in range(int(d / 2)):\n",
    "            denominator = torch.pow(torch.tensor(n, dtype=torch.float32), 2 * i / d)\n",
    "            P[k, 2 * i] = torch.sin(k / denominator)\n",
    "            P[k, 2 * i + 1] = torch.cos(k / denominator)\n",
    "    return P\n",
    "\n",
    "class LearnedSinPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(LearnedSinPositionalEmbedding, self).__init__()\n",
    "         \n",
    "        self.positional_input = getPositionEncoding(seq_len=max_len, d=embedding_dim*2, n=10_000) #10k is from attention is all...\n",
    "        \n",
    "        # Determine positional_emb_dim from positional_input\n",
    "        positional_emb_dim = self.positional_input.shape[1]\n",
    "        \n",
    "        # Linear layer with input size of positional_emb_dim and output size of embedding_dim\n",
    "        self.linear = nn.Linear(positional_emb_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Apply the linear layer to each position\n",
    "        x_add = F.relu(self.linear(self.positional_input))\n",
    "        x_add = x_add.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        \n",
    "        return x + x_add\n",
    "    \n",
    "class SinPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(SinPositionalEmbedding, self).__init__()\n",
    "        \n",
    "        self.positional_input = getPositionEncoding(seq_len=max_len, d=embedding_dim, n=10_000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x_add = self.positional_input.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        return x + x_add\n",
    "    \n",
    "class LearnedDoubleCrossBabyWithSinEmbedding(nn.Module):\n",
    "    \"\"\"2 interactions in the embedding space.  then sums.  that sum could be learned?\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(LearnedDoubleCrossBabyWithSinEmbedding, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.word_pos_emb = SinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = SinPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = SinPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.up_1 = nn.Linear(self.word_embed*5, self.word_embed*10)\n",
    "        self.up_2 = nn.Linear(self.word_embed*10,self.vocab_size)\n",
    "        self.final = nn.Linear(self.sequence_length, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        #print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        #print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.up_1(x))\n",
    "        #print(x.shape, \"up_1\")\n",
    "        x = F.relu(self.up_2(x))\n",
    "        #print(x.shape,\"up_2\")\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.final(x))\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405ea57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 33118413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "}\n",
    "\n",
    "model = LearnedDoubleCrossBabyWithSinEmbedding(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53123edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to LearnedDoubleCrossBabyWithSinEmbedding_conf.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"LearnedDoubleCrossBabyWithSinEmbedding_conf.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3cfacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedDoubleCrossBabyWithLearnedSinEmbedding(nn.Module):\n",
    "    \"\"\"2 interactions in the embedding space.  then sums.  that sum could be learned?\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(LearnedDoubleCrossBabyWithLearnedSinEmbedding, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.up_1 = nn.Linear(self.word_embed*5, self.word_embed*10)\n",
    "        self.up_2 = nn.Linear(self.word_embed*10,self.vocab_size)\n",
    "        self.final = nn.Linear(self.sequence_length, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        #print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        #print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.up_1(x))\n",
    "        #print(x.shape, \"up_1\")\n",
    "        x = F.relu(self.up_2(x))\n",
    "        #print(x.shape,\"up_2\")\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.final(x))\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9104465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 33268813\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "}\n",
    "\n",
    "model = LearnedDoubleCrossBabyWithLearnedSinEmbedding(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdffa0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to LearnedDoubleCrossBabyWithLearnedSinEmbedding_conf.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"LearnedDoubleCrossBabyWithLearnedSinEmbedding_conf.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90713eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "add the memory thing to dcbb..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6571bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareMemory(nn.Module):\n",
    "    def __init__(self,input_dim, memory_dim):\n",
    "        super(SquareMemory, self).__init__()\n",
    "        \n",
    "        self.mem = nn.Parameter(torch.randn(memory_dim,memory_dim))\n",
    "        self.mem.requires_grad = True\n",
    "        \n",
    "        \n",
    "\n",
    "        # Create the final linear layer for output\n",
    "        self.first_axis = nn.Linear(input_dim, memory_dim)\n",
    "        self.second_axis = nn.Linear(input_dim, memory_dim)\n",
    "        self.intercept = nn.Linear(input_dim,memory_dim)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        #batch_dim = input1.shape[0]\n",
    "        #print(batch_dim)\n",
    "        first = F.softmax(self.first_axis(input1), dim=-1)\n",
    "        first = torch.matmul(first, self.mem) #first is used as the lookup, picks a row with the softmax\n",
    "        \n",
    "        second = F.relu(self.second_axis(input1))\n",
    "        \n",
    "        x = second * first\n",
    "        \n",
    "        intercept = F.relu(self.intercept(input1))\n",
    "        x = x + intercept\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e154bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(nn.Module):\n",
    "    \"\"\"Learned Sin embeddings, and square memory, some linear layers at the end.  one projection to the vocab pred\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , linear_dim\n",
    "                ):\n",
    "        super(Model_1, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.linear_dim = int(linear_dim)\n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.down_2 = nn.Linear(self.word_embed*5,self.linear_dim)\n",
    "        \n",
    "        self.flat_down = nn.Linear(self.linear_dim*self.sequence_length,self.linear_dim)\n",
    "        \n",
    "        self.deep_1 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        self.deep_2 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        self.deep_3 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        \n",
    "        self.project = nn.Linear(self.linear_dim,vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.down_2(x))\n",
    "        print(x.shape, \"final down\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.flat_down(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.deep_1(x))\n",
    "        x = F.relu(self.deep_2(x))\n",
    "        x = F.relu(self.deep_3(x))\n",
    "        x = self.project(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a9843b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000])\n",
      "torch.Size([10, 512, 50]) first embedding\n",
      "torch.Size([10, 512, 50]) after embedding\n",
      "torch.Size([10, 512, 512, 50]) after einsum\n",
      "torch.Size([10, 512, 25600]) after reshape\n",
      "torch.Size([10, 512, 100]) after reduce first cross\n",
      "torch.Size([10, 512, 512, 100]) second ein\n",
      "torch.Size([10, 512, 51200]) another reshape\n",
      "torch.Size([10, 512, 250]) second reduce\n",
      "torch.Size([10, 512, 200]) final down\n",
      "torch.Size([10, 102400])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 44193800\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "    , \"linear_dim\":200\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03011ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to Model_x_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"Model_x_config.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "576fbeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000])\n",
      "torch.Size([10, 512, 50]) first embedding\n",
      "torch.Size([10, 512, 50]) after embedding\n",
      "torch.Size([10, 512, 512, 50]) after einsum\n",
      "torch.Size([10, 512, 25600]) after reshape\n",
      "torch.Size([10, 512, 100]) after reduce first cross\n",
      "torch.Size([10, 512, 512, 100]) second ein\n",
      "torch.Size([10, 512, 51200]) another reshape\n",
      "torch.Size([10, 512, 250]) second reduce\n",
      "torch.Size([10, 512, 200]) final down\n",
      "torch.Size([10, 102400])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 44193800\n"
     ]
    }
   ],
   "source": [
    "model = Model_1(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54e068b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    \"\"\"Learned Sin embeddings, and square memory, some linear layers at the end.  one projection to the vocab pred\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , linear_dim\n",
    "                ):\n",
    "        super(Model_2, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.linear_dim = int(linear_dim)\n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.down_2 = nn.Linear(self.word_embed*5,self.linear_dim)\n",
    "        \n",
    "        self.flat_down = nn.Linear(self.linear_dim*self.sequence_length,self.linear_dim)\n",
    "        \n",
    "        self.deep_1 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        self.deep_2 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        self.deep_3 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        \n",
    "        self.project_1 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.project_2 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.project_3 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.down_2(x))\n",
    "        print(x.shape, \"final down\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.flat_down(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.deep_1(x))\n",
    "        p1 = self.project_1(x)\n",
    "        print(p1.shape)\n",
    "        x = F.relu(self.deep_2(x))\n",
    "        p2 = self.project_2(x)\n",
    "        print(p2.shape)\n",
    "        x = F.relu(self.deep_3(x))\n",
    "        p3 = self.project_1(x)\n",
    "        print(p3.shape)\n",
    "        x = p1 + p2 + p3\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2da3bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000])\n",
      "torch.Size([10, 512, 50]) first embedding\n",
      "torch.Size([10, 512, 50]) after embedding\n",
      "torch.Size([10, 512, 512, 50]) after einsum\n",
      "torch.Size([10, 512, 25600]) after reshape\n",
      "torch.Size([10, 512, 100]) after reduce first cross\n",
      "torch.Size([10, 512, 512, 100]) second ein\n",
      "torch.Size([10, 512, 51200]) another reshape\n",
      "torch.Size([10, 512, 250]) second reduce\n",
      "torch.Size([10, 512, 200]) final down\n",
      "torch.Size([10, 102400])\n",
      "torch.Size([10, 200])\n",
      "torch.Size([10, 32000])\n",
      "torch.Size([10, 32000])\n",
      "torch.Size([10, 32000])\n",
      "torch.Size([10, 32000])\n",
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 57057800\n"
     ]
    }
   ],
   "source": [
    "model = Model_2(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a74d51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3(nn.Module):\n",
    "    \"\"\"Learned Sin embeddings, and square memory, some linear layers at the end.  several projections from the same latent\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , linear_dim\n",
    "                ):\n",
    "        super(Model_3, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.linear_dim = int(linear_dim)\n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.down_2 = nn.Linear(self.word_embed*5,self.linear_dim)\n",
    "        \n",
    "        self.flat_down = nn.Linear(self.linear_dim*self.sequence_length,self.linear_dim)\n",
    "\n",
    "        \n",
    "        self.project_1 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.project_2 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.project_3 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.down_2(x))\n",
    "        print(x.shape, \"final down\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.flat_down(x))\n",
    "        \n",
    "        p1 = self.project_1(x)\n",
    "        p2 = self.project_2(x)\n",
    "        p3 = self.project_3(x)\n",
    "        x = p1 + p2 + p3\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b6da94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000])\n",
      "torch.Size([10, 512, 50]) first embedding\n",
      "torch.Size([10, 512, 50]) after embedding\n",
      "torch.Size([10, 512, 512, 50]) after einsum\n",
      "torch.Size([10, 512, 25600]) after reshape\n",
      "torch.Size([10, 512, 100]) after reduce first cross\n",
      "torch.Size([10, 512, 512, 100]) second ein\n",
      "torch.Size([10, 512, 51200]) another reshape\n",
      "torch.Size([10, 512, 250]) second reduce\n",
      "torch.Size([10, 512, 200]) final down\n",
      "torch.Size([10, 102400])\n",
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 56937200\n"
     ]
    }
   ],
   "source": [
    "model = Model_3(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "SquareMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed3cdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_4(nn.Module):\n",
    "    \"\"\"Learned Sin embeddings, and square memory, some linear layers at the end. a memory project and a sentence project\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , linear_dim\n",
    "                ):\n",
    "        super(Model_4, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.linear_dim = int(linear_dim)\n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        self.mem_dim = 500\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed*2)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*2)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed*2,self.word_embed*5)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(sequence_length,self.word_embed*5)\n",
    "        \n",
    "        self.down_2 = nn.Linear(self.word_embed*5,self.linear_dim)\n",
    "        \n",
    "        self.flat_down = nn.Linear(self.linear_dim*self.sequence_length,self.linear_dim)\n",
    "        \n",
    "        self.mem_adjust = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        \n",
    "        self.mem = SquareMemory(self.linear_dim, self.mem_dim)\n",
    "        \n",
    "        self.x_project = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.mem_project = nn.Linear(self.mem_dim,vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.down_2(x))\n",
    "        print(x.shape, \"final down\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.flat_down(x))\n",
    "        \n",
    "        m_x = F.relu(self.mem_adjust(x))\n",
    "        mem = self.mem(m_x)\n",
    "        print(mem.shape)\n",
    "        p1 = self.x_project(x)\n",
    "        p2 = self.mem_project(mem)\n",
    "        \n",
    "        x = p1 + p2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32cadf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000])\n",
      "torch.Size([10, 512, 100]) first embedding\n",
      "torch.Size([10, 512, 100]) after embedding\n",
      "torch.Size([10, 512, 512, 100]) after einsum\n",
      "torch.Size([10, 512, 51200]) after reshape\n",
      "torch.Size([10, 512, 200]) after reduce first cross\n",
      "torch.Size([10, 512, 512, 200]) second ein\n",
      "torch.Size([10, 512, 102400]) another reshape\n",
      "torch.Size([10, 512, 500]) second reduce\n",
      "torch.Size([10, 512, 400]) final down\n",
      "torch.Size([10, 204800])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 177238300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":100\n",
    "    , \"linear_dim\":400\n",
    "}\n",
    "\n",
    "model = Model_4(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa024297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to big_p.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":100\n",
    "    , \"linear_dim\":400\n",
    "}\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"big_p.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10e46968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedDoubleCrossBabyWithLearnedSinEmbedding(nn.Module):\n",
    "    \"\"\"2 interactions in the embedding space.  then sums.  that sum could be learned?\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , first_reduce = None\n",
    "                 , second_reduce = None\n",
    "                 , up_scale = None\n",
    "                ):\n",
    "        super(LearnedDoubleCrossBabyWithLearnedSinEmbedding, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        #deal with defaults for backwards compatibility \n",
    "        #i initially made this so it could scale just by changing the word_embed, thus the multiplication\n",
    "        #now we growing these I want more control so:\n",
    "        \n",
    "        if first_reduce is None:\n",
    "            self.first_reduce = self.word_embed*2\n",
    "        else:\n",
    "            self.first_reduce = int(first_reduce)\n",
    "        \n",
    "        if second_reduce is None:\n",
    "            self.second_reduce = self.word_embed*5\n",
    "        else:\n",
    "            self.second_reduce = int(second_reduce)\n",
    "        \n",
    "        if up_scale is None:\n",
    "            self.up_scale = self.word_embed*10\n",
    "        else:\n",
    "            self.up_scale = int(up_scale)\n",
    "        \n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.first_reduce)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.first_reduce)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.first_reduce,self.second_reduce)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(self.sequence_length,self.second_reduce)\n",
    "        \n",
    "        self.up_1 = nn.Linear(self.second_reduce, self.up_scale)\n",
    "        self.up_2 = nn.Linear(self.up_scale,self.vocab_size)\n",
    "        self.final = nn.Linear(self.sequence_length, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        #print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        #print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.up_1(x))\n",
    "        #print(x.shape, \"up_1\")\n",
    "        x = F.relu(self.up_2(x))\n",
    "        #print(x.shape,\"up_2\")\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.final(x))\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "    \n",
    "    def to(self, device):\n",
    "        self = super().to(device)\n",
    "        self.word_pos_emb.positional_input = self.word_pos_emb.positional_input.to(device)\n",
    "        self.word_pos_emb_reduce.positional_input = self.word_pos_emb_reduce.positional_input.to(device)\n",
    "        self.word_pos_emb_reduce_2.positional_input = self.word_pos_emb_reduce_2.positional_input.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ad537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 382217913\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":200\n",
    "    , \"first_reduce\":500\n",
    "    , \"second_reduce\":1000\n",
    "    , \"up_scale\":2000\n",
    "}\n",
    "\n",
    "model = LearnedDoubleCrossBabyWithLearnedSinEmbedding(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "026d6ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 3_aug_big.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filename = \"3_aug_big.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b909b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    \"\"\"Learned Sin embeddings, and square memory, some linear layers at the end.  one projection to the vocab pred\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , linear_dim\n",
    "                 , first_reduce = None\n",
    "                 , second_reduce = None\n",
    "                 \n",
    "                ):\n",
    "        super(Model_2, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.linear_dim = int(linear_dim)\n",
    "        self.word_pos_emb = LearnedSinPositionalEmbedding(self.sequence_length,self.word_embed)\n",
    "        \n",
    "        if first_reduce is None:\n",
    "            self.first_reduce = self.word_embed*2\n",
    "        else:\n",
    "            self.first_reduce = int(first_reduce)\n",
    "        \n",
    "        if second_reduce is None:\n",
    "            self.second_reduce = self.word_embed*5\n",
    "        else:\n",
    "            self.second_reduce = int(second_reduce)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.first_reduce)\n",
    "        self.word_pos_emb_reduce = LearnedSinPositionalEmbedding(sequence_length,self.first_reduce)\n",
    "        \n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.first_reduce,self.second_reduce)\n",
    "        self.word_pos_emb_reduce_2 = LearnedSinPositionalEmbedding(sequence_length,self.second_reduce)\n",
    "        \n",
    "        self.down_2 = nn.Linear(self.second_reduce,self.linear_dim)\n",
    "        \n",
    "        self.flat_down = nn.Linear(self.linear_dim*self.sequence_length,self.linear_dim)\n",
    "        \n",
    "        self.deep_1 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        self.deep_2 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        self.deep_3 = nn.Linear(self.linear_dim,self.linear_dim)\n",
    "        \n",
    "        self.project_1 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.project_2 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        self.project_3 = nn.Linear(self.linear_dim,vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape, \"first embedding\")\n",
    "        #add pos emb here\n",
    "        x = self.word_pos_emb(x)\n",
    "        #print(x.shape, \"after embedding\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        x = self.word_pos_emb_reduce(x)\n",
    "        #add pos emb here\n",
    "        #print(x.shape, \"after reduce first cross\")\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape,\"second ein\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x.shape, \"another reshape\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        x = self.word_pos_emb_reduce_2(x)\n",
    "        #as pos emb here\n",
    "        #print(x.shape, \"second reduce\")\n",
    "        x = F.relu(self.down_2(x))\n",
    "        #print(x.shape, \"final down\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.flat_down(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.deep_1(x))\n",
    "        p1 = self.project_1(x)\n",
    "        #print(p1.shape)\n",
    "        x = F.relu(self.deep_2(x))\n",
    "        p2 = self.project_2(x)\n",
    "        #print(p2.shape)\n",
    "        x = F.relu(self.deep_3(x))\n",
    "        p3 = self.project_1(x)\n",
    "        #print(p3.shape)\n",
    "        x = p1 + p2 + p3\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def to(self, device):\n",
    "        self = super().to(device)\n",
    "        self.word_pos_emb.positional_input = self.word_pos_emb.positional_input.to(device)\n",
    "        self.word_pos_emb_reduce.positional_input = self.word_pos_emb_reduce.positional_input.to(device)\n",
    "        self.word_pos_emb_reduce_2.positional_input = self.word_pos_emb_reduce_2.positional_input.to(device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c30b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 648502000\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":200\n",
    "    , \"linear_dim\":1000\n",
    "    , \"first_reduce\": 200\n",
    "    , \"second_reduce\": 100\n",
    "}\n",
    "\n",
    "model = Model_2(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c8cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 8_1B_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "filename = \"8_1B_config.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32471b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
