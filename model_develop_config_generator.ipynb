{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc4ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1724762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossBaby_1(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                ):\n",
    "        super(CrossBaby_1, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        self.reduce = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.reduce_2 = nn.Linear(self.sequence_length*self.word_embed,self.word_embed)\n",
    "        self.out = nn.Linear(self.word_embed,self.vocab_size)\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        print(x.shape, \"after einsum\")\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        print(x.shape, \"after reshape\")\n",
    "        x = F.relu(self.reduce(x))\n",
    "        print(x.shape, \"after reduce\")\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        print(x.shape, \"after reshape 2\")\n",
    "        x = F.relu(self.reduce_2(x))\n",
    "        print(x.shape,\"after reduce_2\")\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class CrossBaby_2(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , word_embed\n",
    "                 , up_dim\n",
    "                ):\n",
    "        super(CrossBaby_2, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) \n",
    "        \n",
    "        self.x_a_linear = nn.Linear(self.word_embed*self.sequence_length,self.word_embed)\n",
    "        self.a_down = nn.Linear(self.word_embed*self.sequence_length,self.word_embed )\n",
    "        \n",
    "        self.x_b_linear = nn.Linear(self.word_embed*self.sequence_length,self.word_embed)\n",
    "        self.b_down = nn.Linear(self.word_embed*self.sequence_length,self.word_embed )\n",
    "        \n",
    "        self.x_c_linear = nn.Linear(self.sequence_length*self.sequence_length,self.word_embed)\n",
    "        self.c_down = nn.Linear(self.word_embed*self.word_embed,self.word_embed )\n",
    "        \n",
    "        self.out_1 = nn.Linear(3*self.word_embed,up_dim)\n",
    "        self.out_2 = nn.Linear(up_dim,vocab_size )\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        #print(x.shape)\n",
    "        x = torch.einsum('bij,bkm->bikj', x, x)\n",
    "        #print(x.shape, \"after einsum\")\n",
    "        x_a = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x_a.shape, \"x_a\")\n",
    "        a = F.relu(self.x_a_linear(x_a))\n",
    "        a = a.reshape(a.shape[0],-1)\n",
    "        a = F.relu(self.a_down(a))\n",
    "        #print(a.shape,\"a\")\n",
    "        x_b = x.permute(0,2,1,3).reshape(x.shape[0],x.shape[1],-1)\n",
    "        #print(x_b.shape, \"x_b\")\n",
    "        b = F.relu(self.x_b_linear(x_b))\n",
    "        b = b.reshape(b.shape[0],-1)\n",
    "        b = F.relu(self.b_down(b))\n",
    "        #print(b.shape, \"b\")\n",
    "        x_c = x.permute(0,3,1,2).reshape(x.shape[0],x.shape[-1],-1)\n",
    "        #print(x_c.shape, \"x_c\")\n",
    "        c = F.relu(self.x_c_linear(x_c))\n",
    "        c = c.reshape(c.shape[0],-1)\n",
    "        c = F.relu(self.c_down(c))\n",
    "        #print(c.shape)\n",
    "        d = torch.concat((a,b,c), dim=-1)\n",
    "        #print(d.shape)\n",
    "        d = F.relu(self.out_1(d))\n",
    "        d = self.out_2(d)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ce63327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32000]) torch.Size([10, 32000])\n",
      "Total parameters in DimMemory layer: 36060050\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "    , \"sequence_length\": 512\n",
    "    , \"word_embed\":50\n",
    "    , \"up_dim\":500\n",
    "}\n",
    "\n",
    "model = CrossBaby_2(**kwargs)\n",
    "\n",
    "i = torch.randn((10,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1999d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to CrossBaby_2_50.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"CrossBaby_2_50.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f84b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TransformerBaby(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer model that accepts one-hot encoded input and outputs a tensor of shape batch x vocab_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size\n",
    "                 , sequence_length\n",
    "                 , d_model\n",
    "                 , nhead\n",
    "                 , num_layers\n",
    "                 , dim_feedforward\n",
    "                ):\n",
    "        super(TransformerBaby, self).__init__()\n",
    "\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Linear layer to match d_model size\n",
    "        self.word_embedding = nn.Linear(vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, sequence_length)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.fc = nn.Linear(d_model * sequence_length, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.word_embedding(src)\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        output = self.transformer_encoder(src)\n",
    "        print(output.shape, \"pre-flat\")\n",
    "        output = output.view(output.size(0), -1)  # Flattening\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding that adds position information to input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35576aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to transformer_1.json\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\":32_000\n",
    "                 , \"sequence_length\": 512\n",
    "                 , \"d_model\":64\n",
    "                 , \"nhead\": 2\n",
    "                 , \"num_layers\": 2\n",
    "                 , \"dim_feedforward\":32\n",
    "}\n",
    "\n",
    "model = TransformerBaby(**kwargs)\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"transformer_1.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfca9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 64]) pre-flat\n",
      "torch.Size([2, 512, 32000]) torch.Size([2, 32000])\n",
      "Total parameters in DimMemory layer: 1050698240\n"
     ]
    }
   ],
   "source": [
    "i = torch.randn((2,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb3a26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBaby(nn.Module):\n",
    "    \"\"\"This flattens everything at the end so you have balanced_dim ** 3 in the second to last layer\"\"\"\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim #vestigial lololol\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                ):\n",
    "        super(LlamaBaby, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim) #could apply twice\n",
    "        self.out_down = nn.Linear(self.sentence_embed,self.balanced_dim)\n",
    "        self.out = nn.Linear(self.balanced_dim**3,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = F.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = F.relu(self.out_down(x)) \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class MemoryBaby(nn.Module):\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                 , mem_input_dim\n",
    "                 , mem_hidden_dim\n",
    "                 , mem_output_dim\n",
    "                 , memory_dim\n",
    "                ):\n",
    "        super(MemoryBaby, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim)\n",
    "        self.out_down = nn.Linear(self.sentence_embed,self.balanced_dim)\n",
    "        self.to_mem = nn.Linear(self.balanced_dim**3,mem_input_dim)\n",
    "        self.dim_memory = DimMemory(mem_input_dim,mem_hidden_dim,mem_output_dim,memory_dim)\n",
    "        \n",
    "        self.out = nn.Linear(mem_input_dim + mem_output_dim,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Flatten the sequence_length and vocab_size dimensions\n",
    "        x = torch.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = torch.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = torch.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = torch.relu(self.out_down(x)) #duplicated, could be a second layer.   \n",
    "        \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = torch.relu(self.to_mem(x))\n",
    "        y = self.dim_memory(x)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fce7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"sequence_length\": 512,\n",
    "    \"hidden_dim\": 11,\n",
    "    \"word_embed\": 12,\n",
    "    \"sentence_embed\": 13,\n",
    "    \"balanced_dim\": 14\n",
    "}\n",
    "model = LlamaBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d94b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"sequence_length\": 512,\n",
    "    \"hidden_dim\": 11,\n",
    "    \"word_embed\": 12,\n",
    "    \"sentence_embed\": 13,\n",
    "    \"balanced_dim\": 14\n",
    "}\n",
    "model = LlamaBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9d68242",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'vocab_size': 32_000\n",
    "     , 'sequence_length': 512\n",
    "     , 'hidden_dim': 10\n",
    "     , 'word_embed': 11\n",
    "     , 'sentence_embed': 12\n",
    "     , 'balanced_dim': 13\n",
    "     , 'mem_input_dim': 14 \n",
    "     , 'mem_hidden_dim': 15\n",
    "     , 'mem_output_dim': 16\n",
    "     , 'memory_dim': 4\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = MemoryBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac8208ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_baby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m32_000\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape, \u001b[43mllama_baby\u001b[49m(i)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal parameters in DimMemory layer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llama_baby' is not defined"
     ]
    }
   ],
   "source": [
    "i = torch.randn((2,512,32_000))\n",
    "print(i.shape, llama_baby(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d7cf5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaFetus(nn.Module):\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim #vestigial lololol\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                ):\n",
    "        super(LlamaFetus, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim) #could apply twice\n",
    "    \n",
    "        self.out = nn.Linear(self.balanced_dim**3,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Flatten the sequence_length and vocab_size dimensions\n",
    "        x = F.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = F.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = F.relu(self.seq_down(x)) #duplicated, could be a second layer.   \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        # Pass through the second fully connected layer\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "class DimMemory(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, memory_dim):\n",
    "        super(DimMemory, self).__init__()\n",
    "        if memory_dim < 1:\n",
    "            raise ValueError(\"memory_dim must be greater than or equal to 1.\")\n",
    "        \n",
    "        self.memory_dim = memory_dim \n",
    "        # Create memory tensor with dynamic shape\n",
    "        mem_args = [1]\n",
    "        mem_args.extend([hidden_dim for _ in range(memory_dim)])\n",
    "        self.mem = nn.Parameter(torch.randn(*mem_args))\n",
    "        self.mem.requires_grad = True\n",
    "        \n",
    "        # Create a list of linear layers with memory_dim - 1 repetitions\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(input_dim, hidden_dim) for _ in range(memory_dim - 1)\n",
    "        ])\n",
    "\n",
    "        # Create the final linear layer for output\n",
    "        self.linear_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input1):\n",
    "        # Expand memory tensor along the third dimension\n",
    "        mem_forward_args = [input1.shape[0]]\n",
    "        mem_forward_args.extend([-1 for _ in range(self.memory_dim)])\n",
    "        \n",
    "        x = self.mem.expand(*mem_forward_args)\n",
    "        \n",
    "        for i, linear_layer in enumerate(self.linears):\n",
    "            #print(linear_layer.weight.shape)\n",
    "            \n",
    "            y = torch.relu(linear_layer(input1))\n",
    "            print(y.shape, x.shape)\n",
    "            x = torch.einsum('az,a...yz->a...y',y,x)\n",
    "            #print(x.shape)\n",
    "        # Apply the final linear layer for output\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.linear_out(x))\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class MemoryLlama(nn.Module):\n",
    "    def __init__(self\n",
    "                 , vocab_size\n",
    "                 , sequence_length\n",
    "                 , hidden_dim\n",
    "                 , word_embed\n",
    "                 , sentence_embed\n",
    "                 , balanced_dim\n",
    "                 , mem_input_dim\n",
    "                 , mem_hidden_dim\n",
    "                 , mem_output_dim\n",
    "                 , memory_dim\n",
    "                ):\n",
    "        super(MemoryLlama, self).__init__()\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.word_embed = int(word_embed)\n",
    "        self.sentence_embed = int(sentence_embed)\n",
    "        self.balanced_dim = int(balanced_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Linear(self.vocab_size,self.word_embed) #50 is word embeddings essentially\n",
    "        self.sentence_embedding = nn.Linear(self.sequence_length,self.sentence_embed) #also could apply twice.\n",
    "        self.we_down = nn.Linear(self.word_embed,self.balanced_dim)\n",
    "        self.seq_down = nn.Linear(self.sentence_embed,self.balanced_dim) #could apply twice\n",
    "        self.to_mem = nn.Linear(self.balanced_dim**3,mem_input_dim)\n",
    "        self.dim_memory = DimMemory(mem_input_dim,mem_hidden_dim,mem_output_dim,memory_dim)\n",
    "        \n",
    "        self.out = nn.Linear(mem_input_dim + mem_output_dim,self.vocab_size)\n",
    "        # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.word_embedding(x)) #sentence of word embeddings.  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.sentence_embedding(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Flatten the sequence_length and vocab_size dimensions\n",
    "        x = torch.relu(torch.einsum('bij,bkm->bikj', x, x)) #interactions\n",
    "        x = torch.relu(self.we_down(x))\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = torch.relu(self.seq_down(x))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = torch.relu(self.seq_down(x)) #duplicated, could be a second layer.   \n",
    "        \n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = torch.relu(self.to_mem(x))\n",
    "        y = self.dim_memory(x)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "397a0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'vocab_size': 32_000\n",
    "     , 'sequence_length': 512\n",
    "     , 'hidden_dim': 100\n",
    "     , 'word_embed': 50\n",
    "     , 'sentence_embed': 200\n",
    "     , 'balanced_dim': 40\n",
    "     , 'mem_input_dim': 20 \n",
    "     , 'mem_hidden_dim': 40\n",
    "     , 'mem_output_dim': 100\n",
    "     , 'memory_dim': 4\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = MemoryBaby(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "006b6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40]) torch.Size([1, 40, 40, 40, 40])\n",
      "torch.Size([1, 40]) torch.Size([1, 40, 40, 40])\n",
      "torch.Size([1, 40]) torch.Size([1, 40, 40])\n",
      "torch.Size([1, 120])\n",
      "torch.Size([1, 512, 32000]) torch.Size([1, 32000])\n",
      "Total parameters in DimMemory layer: 9439410\n"
     ]
    }
   ],
   "source": [
    "i = torch.randn((1,512,32_000))\n",
    "print(i.shape, model(i).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d595a654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9439410"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1efe7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to membaby_small_9mil.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = \"membaby_small_9mil.json\"\n",
    "\n",
    "# Using json.dump() to save the dictionary to a JSON file\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(kwargs, json_file)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_layer = DimMemory(input_dim = 200,hidden_dim=11, output_dim=20,memory_dim=6)\n",
    "\n",
    "# Assuming you have an input tensor input1 with shape (batch_size, 10)\n",
    "input1 = torch.ones(4,200)\n",
    "out2 = memory_layer(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in memory_layer.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    memory_layer = DimMemory(input_dim = 20,hidden_dim=10, output_dim=20,memory_dim=i)\n",
    "    input1 = torch.ones(4,20)\n",
    "    out2 = memory_layer(input1)\n",
    "    total_params = sum(p.numel() for p in memory_layer.parameters())\n",
    "    formatted_total_params = '{:,}'.format(total_params)\n",
    "    print(\"Memory_dim=\",i,\"Total parameters in DimMemory layer:\", formatted_total_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89120f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"sequence_length\": 512,\n",
    "    \"hidden_dim\": None,\n",
    "    \"word_embed\": 100,\n",
    "    \"sentence_embed\": 1000,\n",
    "    \"balanced_dim\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8d3e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in DimMemory layer: 259767140\n"
     ]
    }
   ],
   "source": [
    "model = LlamaFetus(**kwargs)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters in DimMemory layer:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706895be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
