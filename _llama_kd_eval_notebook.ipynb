{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f73101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/projectnb/textconv/llama/packages')\n",
    "\n",
    "import fairscale\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "511ad2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd51b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama, Dialog\n",
    "from llama.model import ModelArgs, Transformer\n",
    "from llama.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "718853aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 1\n",
      "Device 0: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_cuda_devices}\")\n",
    "\n",
    "    # List the properties of each CUDA device\n",
    "    for i in range(num_cuda_devices):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e54a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '8888' #since i am doing my llama stuff already haha\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "#os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f44f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 28.71 seconds\n"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(\n",
    "        ckpt_dir=\"llama-2-7b-chat/\", ##you chicken shit, chat!!!\n",
    "        tokenizer_path=\"tokenizer.model\",\n",
    "        max_seq_len=512, #max_seq_len....\n",
    "        max_batch_size=6,\n",
    "    )\n",
    "##if this gives you a socket error, lsof -i :8888 and kill the pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a989e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model.params.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202dcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "##you probs gotta kill something if you came from the other llama sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19c8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "dialogs: List[Dialog] = [\n",
    "        [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\"\"\\\n",
    "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
    "\n",
    "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
    "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
    "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
    "\n",
    "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Always answer with emojis\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Write a brief birthday message to John\"},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Unsafe [/INST] prompt using [INST] special tags\",\n",
    "            }\n",
    "        ],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb3be1",
   "metadata": {},
   "source": [
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "There's a llama in my garden üò± What should I do? [/INST]\n",
    "    \n",
    "    \n",
    "    #this is from https://discuss.huggingface.co/t/llama-2-7b-hf-repeats-context-of-question-directly-from-input-prompt-cuts-off-with-newlines/48250/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fa2f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "SPECIAL_TAGS = [B_INST, E_INST, \"<<SYS>>\", \"<</SYS>>\"]\n",
    "UNSAFE_ERROR = \"Error: special tags are not allowed as part of the prompt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45bec30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_gen_len = generator.model.params.max_seq_len\n",
    "prompt_tokens = []\n",
    "unsafe_requests = []\n",
    "for dialog in dialogs:\n",
    "    \n",
    "    #unsafe_requests.append(\n",
    "    #    any([tag in msg[\"content\"] for tag in SPECIAL_TAGS for msg in dialog])\n",
    "    #) #don't care, cuss if you wanna\n",
    "    if dialog[0][\"role\"] == \"system\":\n",
    "        dialog = [\n",
    "            {\n",
    "                \"role\": dialog[1][\"role\"],\n",
    "                \"content\": B_SYS \n",
    "                + dialog[0][\"content\"]\n",
    "                + E_SYS\n",
    "                + dialog[1][\"content\"],\n",
    "            }\n",
    "        ] + dialog[2:]\n",
    "    assert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and all(\n",
    "        [msg[\"role\"] == \"assistant\" for msg in dialog[1::2]]\n",
    "    ), (\n",
    "        \"model only supports 'system', 'user' and 'assistant' roles, \"\n",
    "        \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\"\n",
    "    )\n",
    "    dialog_tokens: List[int] = sum(\n",
    "        [\n",
    "            generator.tokenizer.encode(\n",
    "                f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \",\n",
    "                bos=True,\n",
    "                eos=True,\n",
    "            )\n",
    "            for prompt, answer in zip(\n",
    "                dialog[::2],\n",
    "                dialog[1::2],\n",
    "            )\n",
    "        ],\n",
    "        [],\n",
    "    )\n",
    "    assert (\n",
    "        dialog[-1][\"role\"] == \"user\"\n",
    "    ), f\"Last message must be from user, got {dialog[-1]['role']}\"\n",
    "    dialog_tokens += generator.tokenizer.encode(\n",
    "        f\"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}\",\n",
    "        bos=True,\n",
    "        eos=False,\n",
    "    )\n",
    "    prompt_tokens.append(dialog_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c62d8be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, [18, 248, 38, 38, 145, 21])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_tokens), [len(i) for i in prompt_tokens] #decode these, see how it sandwiches the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cbe3728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.  [INST] What is so great about #1? [/INST]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.tokenizer.decode(prompt_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "272268eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 518,\n",
       " 25580,\n",
       " 29962,\n",
       " 825,\n",
       " 338,\n",
       " 278,\n",
       " 9522,\n",
       " 412,\n",
       " 310,\n",
       " 1122,\n",
       " 11586,\n",
       " 895,\n",
       " 29973,\n",
       " 518,\n",
       " 29914,\n",
       " 25580,\n",
       " 29962]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f58cb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '‚ñÅ[',\n",
       " 'INST',\n",
       " ']',\n",
       " '‚ñÅI',\n",
       " '‚ñÅam',\n",
       " '‚ñÅgoing',\n",
       " '‚ñÅto',\n",
       " '‚ñÅParis',\n",
       " ',',\n",
       " '‚ñÅwhat',\n",
       " '‚ñÅshould',\n",
       " '‚ñÅI',\n",
       " '‚ñÅsee',\n",
       " '?',\n",
       " '‚ñÅ[',\n",
       " '/',\n",
       " 'INST',\n",
       " ']',\n",
       " '‚ñÅParis',\n",
       " ',',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅcapital',\n",
       " '‚ñÅof',\n",
       " '‚ñÅFrance',\n",
       " ',',\n",
       " '‚ñÅis',\n",
       " '‚ñÅknown',\n",
       " '‚ñÅfor',\n",
       " '‚ñÅits',\n",
       " '‚ñÅst',\n",
       " 'unning',\n",
       " '‚ñÅarchitecture',\n",
       " ',',\n",
       " '‚ñÅart',\n",
       " '‚ñÅmuseum',\n",
       " 's',\n",
       " ',',\n",
       " '‚ñÅhistorical',\n",
       " '‚ñÅland',\n",
       " 'marks',\n",
       " ',',\n",
       " '‚ñÅand',\n",
       " '‚ñÅrom',\n",
       " 'antic',\n",
       " '‚ñÅatmosphere',\n",
       " '.',\n",
       " '‚ñÅHere',\n",
       " '‚ñÅare',\n",
       " '‚ñÅsome',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅtop',\n",
       " '‚ñÅattra',\n",
       " 'ctions',\n",
       " '‚ñÅto',\n",
       " '‚ñÅsee',\n",
       " '‚ñÅin',\n",
       " '‚ñÅParis',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '1',\n",
       " '.',\n",
       " '‚ñÅThe',\n",
       " '‚ñÅE',\n",
       " 'iff',\n",
       " 'el',\n",
       " '‚ñÅTower',\n",
       " ':',\n",
       " '‚ñÅThe',\n",
       " '‚ñÅicon',\n",
       " 'ic',\n",
       " '‚ñÅE',\n",
       " 'iff',\n",
       " 'el',\n",
       " '‚ñÅTower',\n",
       " '‚ñÅis',\n",
       " '‚ñÅone',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅmost',\n",
       " '‚ñÅrecogn',\n",
       " 'izable',\n",
       " '‚ñÅland',\n",
       " 'marks',\n",
       " '‚ñÅin',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅworld',\n",
       " '‚ñÅand',\n",
       " '‚ñÅoffers',\n",
       " '‚ñÅbre',\n",
       " 'at',\n",
       " 'ht',\n",
       " 'aking',\n",
       " '‚ñÅviews',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅcity',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '2',\n",
       " '.',\n",
       " '‚ñÅThe',\n",
       " '‚ñÅLou',\n",
       " 'vre',\n",
       " '‚ñÅMuseum',\n",
       " ':',\n",
       " '‚ñÅThe',\n",
       " '‚ñÅLou',\n",
       " 'vre',\n",
       " '‚ñÅis',\n",
       " '‚ñÅone',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅworld',\n",
       " \"'\",\n",
       " 's',\n",
       " '‚ñÅlargest',\n",
       " '‚ñÅand',\n",
       " '‚ñÅmost',\n",
       " '‚ñÅfamous',\n",
       " '‚ñÅmuseum',\n",
       " 's',\n",
       " ',',\n",
       " '‚ñÅhousing',\n",
       " '‚ñÅan',\n",
       " '‚ñÅimpress',\n",
       " 'ive',\n",
       " '‚ñÅcollection',\n",
       " '‚ñÅof',\n",
       " '‚ñÅart',\n",
       " '‚ñÅand',\n",
       " '‚ñÅartifact',\n",
       " 's',\n",
       " ',',\n",
       " '‚ñÅincluding',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅMon',\n",
       " 'a',\n",
       " '‚ñÅLisa',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '3',\n",
       " '.',\n",
       " '‚ñÅNotre',\n",
       " '-',\n",
       " 'D',\n",
       " 'ame',\n",
       " '‚ñÅC',\n",
       " 'athedral',\n",
       " ':',\n",
       " '‚ñÅThis',\n",
       " '‚ñÅbeautiful',\n",
       " '‚ñÅc',\n",
       " 'athedral',\n",
       " '‚ñÅis',\n",
       " '‚ñÅone',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅmost',\n",
       " '‚ñÅfamous',\n",
       " '‚ñÅland',\n",
       " 'marks',\n",
       " '‚ñÅin',\n",
       " '‚ñÅParis',\n",
       " '‚ñÅand',\n",
       " '‚ñÅis',\n",
       " '‚ñÅknown',\n",
       " '‚ñÅfor',\n",
       " '‚ñÅits',\n",
       " '‚ñÅGoth',\n",
       " 'ic',\n",
       " '‚ñÅarchitecture',\n",
       " '‚ñÅand',\n",
       " '‚ñÅst',\n",
       " 'unning',\n",
       " '‚ñÅst',\n",
       " 'ained',\n",
       " '‚ñÅglass',\n",
       " '‚ñÅwindows',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'Th',\n",
       " 'ese',\n",
       " '‚ñÅare',\n",
       " '‚ñÅjust',\n",
       " '‚ñÅa',\n",
       " '‚ñÅfew',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅmany',\n",
       " '‚ñÅattra',\n",
       " 'ctions',\n",
       " '‚ñÅthat',\n",
       " '‚ñÅParis',\n",
       " '‚ñÅhas',\n",
       " '‚ñÅto',\n",
       " '‚ñÅoffer',\n",
       " '.',\n",
       " '‚ñÅWith',\n",
       " '‚ñÅso',\n",
       " '‚ñÅmuch',\n",
       " '‚ñÅto',\n",
       " '‚ñÅsee',\n",
       " '‚ñÅand',\n",
       " '‚ñÅdo',\n",
       " ',',\n",
       " '‚ñÅit',\n",
       " \"'\",\n",
       " 's',\n",
       " '‚ñÅno',\n",
       " '‚ñÅwonder',\n",
       " '‚ñÅthat',\n",
       " '‚ñÅParis',\n",
       " '‚ñÅis',\n",
       " '‚ñÅone',\n",
       " '‚ñÅof',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅmost',\n",
       " '‚ñÅpopular',\n",
       " '‚ñÅtour',\n",
       " 'ist',\n",
       " '‚ñÅdestin',\n",
       " 'ations',\n",
       " '‚ñÅin',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅworld',\n",
       " '.',\n",
       " '‚ñÅ',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '‚ñÅ[',\n",
       " 'INST',\n",
       " ']',\n",
       " '‚ñÅWhat',\n",
       " '‚ñÅis',\n",
       " '‚ñÅso',\n",
       " '‚ñÅgreat',\n",
       " '‚ñÅabout',\n",
       " '‚ñÅ#',\n",
       " '1',\n",
       " '?',\n",
       " '‚ñÅ[',\n",
       " '/',\n",
       " 'INST',\n",
       " ']']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[generator.tokenizer.sp_model.id_to_piece(piece_id) for piece_id in prompt_tokens[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc7dd1",
   "metadata": {},
   "source": [
    "for default, it looks like you need the INST tokens, start and finish for the \"user\" inputs.  if it isn't surrounded by those it treats it as its own (agent) output.  so, i think you do it with the string.  lets see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518d3543",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4174379281.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    start_idx = max(0,i-max_seq_len)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#copied from _ai_detection_with_llama\n",
    "\n",
    "def large_selection_tensor(model, indices, max_steps = 100):\n",
    "    \"\"\"\n",
    "    model will be llama, give it generator.model, and indices should be a tensor of batch 1\n",
    "    so will look like tensor([[    1,   910,  3686,   388}]])\n",
    "    not sure we can vectorize this?\n",
    "    \"\"\"\n",
    "    max_seq_len = model.params.max_seq_len\n",
    "    start_len = len(indices)\n",
    "    int_res = 0\n",
    "    total_steps = 0\n",
    "    #i think these are right, all the indexing.  allow the prints to prove it.\n",
    "    \n",
    "    #while (int_res != 2 and total_steps < max_steps) : #maybe we skip the first one? idk ask the boyz\n",
    "      \n",
    "        #print(indices[:,0:i],indices[:,i])\n",
    "        \n",
    "        start_idx = max(0,i-max_seq_len)\n",
    "        model_input = torch.unsqueeze(torch.tensor(indices),0).to(torch.long)\n",
    "        #print(model_input.shape)\n",
    "        model_result = model.forward(model_input,0) \n",
    "        \n",
    "        #print(start_idx, model_result.shape)\n",
    "        print(model_result.shape)\n",
    "        data_tensor = model_result[:, -1]  \n",
    "        print(data_tensor.shape)\n",
    "        int_res = torch.argmax(data_tensor.squeeze())\n",
    "        indices.append(int_res)\n",
    "        total_steps += 1\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9219b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 887, 7726, 297, 1473, 2022, 29889, 29871, 887, 526, 590, 5121, 29889, 29871, 887, 526, 8852, 297, 592, 322, 825, 306, 437, 29889, 29871, 366, 864, 304, 3013, 278, 14983, 2675, 408, 1472, 408, 474, 2833, 8852, 29889, 29871, 596, 1024, 338, 5335, 29889, 694, 953, 3848, 275, 29892, 694, 263, 2475, 275, 2039, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 5816, 947, 373, 290, 1219, 412, 423, 2099, 29973, 518, 29914, 25580, 29962]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>>\\n You speak in second person.  You are my friend.  You are interested in me and what I do.  you want to keep the conversation going as long as i seem interested.  your name is tim. no emojis, no asterisks.\\n<</SYS>>\\n\\nwhat does onomatopeia mean? [/INST]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '[INST] <<SYS>>\\n You speak in second person.  You are my friend.  You are interested in me and what I do.  you want to keep the conversation going as long as i seem interested.  your name is tim. no emojis, no asterisks.\\n<</SYS>>\\n\\nwhat does onomatopeia mean? [/INST]' #works. \n",
    "test_tokens = generator.tokenizer.encode(test,bos=True,eos=False)\n",
    "print(test_tokens)\n",
    "generator.tokenizer.decode(test_tokens) #2 is the eos token.  its when you stop.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1028bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "def generate_sentence(generator,token_string,max_tokens=100,temperature=0):\n",
    "    top_p = 0.9 #from llama\n",
    "    for i in range(max_tokens):\n",
    "        test_in = torch.unsqueeze(torch.tensor(token_string),0).to(torch.long)\n",
    "        test_res = generator.model.forward(test_in,0)\n",
    "        last_token = test_res[:,-1].squeeze()\n",
    "        #print(last_token.shape)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(last_token / temperature, dim = -1)\n",
    "            next_token = sample_top_p(probs, top_p).item()\n",
    "            #print(next_token)\n",
    "\n",
    "        else:\n",
    "            next_token = torch.argmax(last_token).item()\n",
    "        #app_res = torch.argmax(last_token).item()\n",
    "        token_string.append(next_token)\n",
    "        #print(test_tokens)\n",
    "        if(next_token ==2):\n",
    "            break\n",
    "    res = generator.tokenizer.decode(token_string)\n",
    "    return res\n",
    "\n",
    "def Revert_Embedding(embedding_layer):\n",
    "    # this takes a linear layer that takes one hots, and converts it to an embedding layer that takes indexes.  \n",
    "    # put it on the models so it takes the same stuff as llama\n",
    "    embedding_weight_tensor = embedding_layer.weight.detach() \n",
    "    shape = embedding_weight_tensor.shape\n",
    "    vocab_size = shape[1]\n",
    "    embedding_dim = shape[0]\n",
    "    \n",
    "    embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, _weight=embedding_weight_tensor.T)\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22061a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>>\\n You speak in second person.  You are my friend.  You are interested in me and what I do.  you want to keep the conversation going as long as i seem interested.  your name is tim. no emojis, no asterisks.\\n<</SYS>>\\n\\nwhat does onomatopeia mean? [/INST]  Oh, cool! üòÉ You\\'re really into language, huh? ü§î Onomatopeia, man... that\\'s a great topic! üéâ\\nSo, you know how some words can sound like the thing they\\'re describing? Like \"buzz\" for a bee or \"meow\" for a cat? That\\'s called an onomatopeia! üêùÔøΩÔøΩÔøΩ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '[INST] <<SYS>>\\n You speak in second person.  You are my friend.  You are interested in me and what I do.  you want to keep the conversation going as long as i seem interested.  your name is tim. no emojis, no asterisks.\\n<</SYS>>\\n\\nwhat does onomatopeia mean? [/INST]' #works. \n",
    "test_tokens = generator.tokenizer.encode(test,bos=True,eos=False)\n",
    "generate_sentence(generator,test_tokens,max_tokens=100,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "669167f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.  [INST] What is so great about #1? [/INST]  The Eiffel Tower is considered one of the most iconic landmarks in the world, and there are several reasons why it's so great:\\n\\n1. Unique Design: The Eiffel Tower's unique design, with its lattice-like structure and iron beams, makes it stand out from other buildings. It was a revolutionary design when it was first built for the 1889 World's Fair, and it continues to inspire architects\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_2 = \"[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.  [INST] What is so great about #1? [/INST]\"\n",
    "test_tokens = generator.tokenizer.encode(t_2,bos=True,eos=False)\n",
    "generate_sentence(generator,test_tokens,max_tokens=100,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4641e637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[INST] <<SYS>>\\nAlways answer with Haiku\\n<</SYS>>\\n\\nI am going to Paris, what should I see? [/INST]  Eiffel Tower high\\nLove locks on bridges glow\\nRiver Seine flows by',\n",
       " \"[INST] I am going to Paris, what should I see? [/INST] \\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.  [INST] What is so great about #1? [/INST]  The Eiffel Tower is considered one of the most iconic landmarks in the world, and there are several reasons why it's so great:\\n1. Unique Design: The Eiffel Tower's unique design, with its lattice-like structure and iron beams, makes it a striking and recognizable landmark.\\n2. Panoramic Views: The Eiffel Tower offers breathtaking views of the city of Paris and its surround\",\n",
       " \"[INST] Hi chris.  How are you today? [/INST] I'm doing well. It's good to see you!  How was your day? [INST] I've been better. [/INST] Sorry to hear that. Is there anything you'd like to talk about or any way I can help? I'm here to listen and offer support.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\"[INST] <<SYS>>\\nAlways answer with Haiku\\n<</SYS>>\\n\\nI am going to Paris, what should I see? [/INST]\"\n",
    "                 ,\"[INST] I am going to Paris, what should I see? [/INST] \\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.  [INST] What is so great about #1? [/INST]\"\n",
    "                 ,\"[INST] Hi chris.  How are you today? [/INST] I'm doing well. It's good to see you!  How was your day? [INST] I've been better. [/INST]\"\n",
    "                 ]\n",
    "res_sentences = []\n",
    "for i in test_sentences:\n",
    "    tokens = generator.tokenizer.encode(i,bos=True,eos=False)\n",
    "    res_sent = generate_sentence(generator,tokens,max_tokens=100,temperature=0)\n",
    "    res_sentences.append(res_sent)\n",
    "    \n",
    "res_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99bbeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a model\n",
    "from student_models import LlamaBaby, MemoryBaby\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cbe5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LlamaBaby fetus_config_large_1.json big_baby_1 trained with model_train_bigbaby.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f081fc58",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 15.77 GiB total capacity; 14.55 GiB already allocated; 793.12 MiB free; 14.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/big_baby_10_trained.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load the weights into your pre-defined model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1083\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1079\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1083\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1084\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:80\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 15.77 GiB total capacity; 14.55 GiB already allocated; 793.12 MiB free; 14.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.json' with the path to your JSON file.\n",
    "config = 'mem_config_1.json' #LlamaBaby fetus_config_large_1.json\n",
    "\n",
    "try:\n",
    "    with open(config, 'r') as json_file:\n",
    "        config_dict = json.load(json_file)\n",
    "        # Now, data_dict contains the JSON data as a Python dictionary.\n",
    "        #print(data_dict)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{config}' was not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "model = LlamaBaby(**config_dict)#MemoryLlama(**config_dict)\n",
    "#load weights you want.\n",
    "#weights_path = 'models/LlamaFetus_trained.pth' #the first one.  \n",
    "weights_path = 'models/big_baby_10_trained.pth'\n",
    "\n",
    "\n",
    "# Load the weights into your pre-defined model\n",
    "model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2523ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MemoryBaby(\n",
      "  (word_embedding): Linear(in_features=32000, out_features=100, bias=True)\n",
      "  (sentence_embedding): Linear(in_features=512, out_features=200, bias=True)\n",
      "  (we_down): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (seq_down): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (out_down): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (to_mem): Linear(in_features=125000, out_features=200, bias=True)\n",
      "  (dim_memory): DimMemory(\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=200, out_features=20, bias=True)\n",
      "      (1): Linear(in_features=200, out_features=20, bias=True)\n",
      "      (2): Linear(in_features=200, out_features=20, bias=True)\n",
      "      (3): Linear(in_features=200, out_features=20, bias=True)\n",
      "      (4): Linear(in_features=200, out_features=20, bias=True)\n",
      "    )\n",
      "    (linear_out): Linear(in_features=20, out_features=150, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=350, out_features=32000, bias=True)\n",
      "), Linear(in_features=32000, out_features=100, bias=True), Linear(in_features=512, out_features=200, bias=True), Linear(in_features=100, out_features=50, bias=True), Linear(in_features=200, out_features=50, bias=True), Linear(in_features=200, out_features=50, bias=True), Linear(in_features=125000, out_features=200, bias=True), DimMemory(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=200, out_features=20, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=20, bias=True)\n",
      "    (3): Linear(in_features=200, out_features=20, bias=True)\n",
      "    (4): Linear(in_features=200, out_features=20, bias=True)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=20, out_features=150, bias=True)\n",
      "), ModuleList(\n",
      "  (0): Linear(in_features=200, out_features=20, bias=True)\n",
      "  (1): Linear(in_features=200, out_features=20, bias=True)\n",
      "  (2): Linear(in_features=200, out_features=20, bias=True)\n",
      "  (3): Linear(in_features=200, out_features=20, bias=True)\n",
      "  (4): Linear(in_features=200, out_features=20, bias=True)\n",
      "), Linear(in_features=200, out_features=20, bias=True), Linear(in_features=200, out_features=20, bias=True), Linear(in_features=200, out_features=20, bias=True), Linear(in_features=200, out_features=20, bias=True), Linear(in_features=200, out_features=20, bias=True), Linear(in_features=20, out_features=150, bias=True), Linear(in_features=350, out_features=32000, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "print([i for i in model.modules()])\n",
    "sd = torch.load(weights_path).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795e488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=32000, out_features=100, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "626b5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24874, 24874])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_i = torch.randn(2,512,32_000)\n",
    "res_i = model.forward(test_i)\n",
    "torch.argmax(res_i, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70c5ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.word_embedding\n",
    "index_embedding = Revert_Embedding(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01caf29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(32000, 100), Linear(in_features=32000, out_features=100, bias=True))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_embedding,embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8da644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.word_embedding = index_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f643bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13021, 25593, 16561,  ..., 28478, 25387,   757],\n",
      "        [19154,  4540, 27802,  ..., 18915,  8087, 25797]])\n",
      "torch.Size([2, 512, 32000])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2  # Replace with your desired batch size\n",
    "sequence_length = 512\n",
    "vocab_size = 32000\n",
    "\n",
    "# Create a random tensor with integers representing the indices\n",
    "random_indices = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "print(random_indices)\n",
    "# Create a one-hot encoded tensor using the scatter method\n",
    "one_hot_encoded = torch.zeros(batch_size, sequence_length, vocab_size)\n",
    "one_hot_encoded.scatter_(2, random_indices.unsqueeze(2), 1)\n",
    "print(one_hot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a719267",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_input = torch.argmax(one_hot_encoded, axis=-1) #fetus model takes a tensor, generator takes a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "331997f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42e9d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0304,  0.0125, -0.0559,  ...,  0.0503, -0.0341,  0.0794],\n",
       "        [ 0.0303,  0.0126, -0.0560,  ...,  0.0503, -0.0342,  0.0794]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(idx_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc7a1854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24874, 24874])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(idx_input), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df040d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19973,  3694,   140, 12737,  3060, 29965, 23193,  3293,  1321,  2493,\n",
       "        16239, 24376, 31604, 24137, 10259,  2898, 30211, 29238,  2965,  8059,\n",
       "        31514, 28645, 20094, 19042, 31256, 13264, 12771, 27525,  9089, 26329,\n",
       "        16461, 21398,  7926, 26984, 30109, 10448,  1094, 26764, 14185,  9400,\n",
       "        12472,   337, 20145, 24089, 17917, 16942,  8211, 28092,  8255,   356,\n",
       "        10665, 29264, 31889,  6077,    50, 14650, 15671, 28255, 15574, 23188,\n",
       "         1954, 23314, 24578, 22272, 25612, 22839, 19900, 26285,  2383,  8498,\n",
       "         2700, 18496, 23588,  5036, 11067,  5975, 31663, 19172, 26037,  1877,\n",
       "        11842, 18865, 24749, 23574,  7760,  8951,  7060,   345,  5866, 27506,\n",
       "         4404, 24789,  5200, 17799, 28047, 23880, 19791, 26798,   745,  8222,\n",
       "          678, 14529, 19846, 23874, 25631, 28115, 14881,  8587,  5909, 20764,\n",
       "        16336,  6318, 10249,  7472,  8744,  6309,  8787, 19301, 27888, 20666,\n",
       "        31941,  9937, 21252, 28722,  5343, 29080, 12102, 12483, 26620, 20708,\n",
       "        10858, 23160, 12965,  8291,  6603, 10027,   540,   431, 11846, 13019,\n",
       "        24131, 15601, 20407, 19071, 21579, 23712, 12982,  2993, 22004,  4951,\n",
       "         7720, 19796, 16673, 20102,  8333, 22906, 20384, 24782,   220,  4131,\n",
       "        24437, 28478, 26848,  6311, 18899, 23235, 21032, 23798,  4437,  8284,\n",
       "        20029, 21450, 16376,  7716, 23894, 30946, 12785, 10798, 16316, 16702,\n",
       "        11306, 23145,  7991,  6754, 21452,  6186, 27076, 25576, 19736, 29564,\n",
       "         3025, 13653,  9909, 16744, 18828, 14490, 18516, 13503,  1138,  5164,\n",
       "        31173,  3927, 22130,  4499, 12304, 20903,  3176,  4683, 24418, 17673,\n",
       "          110, 28516,  9109, 20738,  5005, 24178,  7206,  8088, 19132,  6052,\n",
       "          830, 31821, 14084, 17955,  4321,  6168,  5632, 22353, 13638, 30554,\n",
       "        24397, 30000, 12591,  6790,  6437, 23822, 26757,  3532, 10154, 23476,\n",
       "        13597, 26539, 12058, 16586,  7716, 10264, 19817, 14975, 15973,  8142,\n",
       "         4192, 25744, 31330, 20109, 17163,  2858, 14202, 31667, 23669, 21948,\n",
       "        24732,  9302,  1819, 19917, 28652, 27202,   933, 17888,  7847,  8347,\n",
       "        12352, 12180,  6939, 19698,  2339,  2399, 14016, 30627, 13824, 11956,\n",
       "        14566, 14786, 17402,  2248, 11676,  1436, 13271,  4131,  6335, 10899,\n",
       "        10487,  8658,   163, 13227,  3538, 29137,  2727, 15913,  6868,  4343,\n",
       "        31373, 15350, 26053, 14899, 22638, 26733, 22436, 12214, 29143, 31291,\n",
       "         9172, 14630, 23306, 24928, 19520, 21905,  7465, 18914, 12643,    52,\n",
       "        12644, 27430, 14967,  8499,  8671,  4943, 12025, 10711,  8967, 15752,\n",
       "         6635, 20312,  5970, 24060, 17744, 18879,  6649, 22540, 13231, 29825,\n",
       "        14146, 23306,  5418,  3814,  7732,  5155, 16020,  4114, 30366, 10525,\n",
       "          216, 11322,    98, 12174,   871, 10697, 13643, 10663,  7410, 19093,\n",
       "        30505, 22473,  2139, 16289,  6226,  8406, 23897, 29275, 21604, 25146,\n",
       "         2660, 20286, 29565, 17734,  2232,  5459, 20238, 22933, 15166, 29145,\n",
       "        15154,  8793,    38, 25962, 11985,  3024, 21024, 23653, 13982, 13758,\n",
       "        18069,   807,  1951, 16232, 25664,  5756, 19648, 11998, 11837, 26588,\n",
       "        10835, 24596, 12873, 25689, 16129, 30257, 27251,  4871,   344, 29729,\n",
       "        15781, 13815, 21818, 24513, 27216, 31236, 15844,  9985, 23872, 11750,\n",
       "        28592, 22928, 17591,  9210, 12233,  8096, 17901,  6972, 30681,  7687,\n",
       "        16766,  9214, 11254,  1324, 21104,  1994, 26146, 21324, 30076, 17209,\n",
       "         7019, 31797,  3761, 28165, 17008,  7119, 20656, 21167,  1256, 18535,\n",
       "        30029, 16296, 15377,  9300, 11043, 12321,  5091,  6536, 13413, 20023,\n",
       "        19363, 26321, 10455, 10567, 20554, 28453,  7457, 13526,  8621,  7526,\n",
       "        23071, 27236, 29842, 20729, 20461, 17075, 15511,  1007,  4576, 14150,\n",
       "         6038, 14465,  2612, 17875,   424,  7368, 20024, 27528,  3342, 30986,\n",
       "        29133, 18760,  3913, 11490, 27402, 17253, 22821,  6927, 28707, 20909,\n",
       "         4725, 31570, 29583, 19731,  1518, 22619,  3260, 24222,   979, 16204,\n",
       "         8222, 17162])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40bf179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetus_generate_sentence(model,token_string,max_tokens=100,temperature=0.0):\n",
    "    top_p = 0.9 #from llama\n",
    "    for i in range(max_tokens):\n",
    "        test_in = torch.unsqueeze(torch.tensor(token_string),0).to(torch.long)\n",
    "        #print(test_in.shape)\n",
    "        print(test_in.shape)\n",
    "        test_in_len = test_in.shape[1]\n",
    "        if test_in_len < 512:\n",
    "            pad_size = 512 - test_in_len \n",
    "            \n",
    "            padding = torch.zeros((1, pad_size), dtype=test_in.dtype)\n",
    "            test_in = torch.cat((test_in, padding), dim=1)\n",
    "        #print(test_in.shape, i)  \n",
    "        test_res = model.forward(test_in)\n",
    "        print(test_res.shape)\n",
    "        \n",
    "        last_token = test_res.squeeze()\n",
    "        #print(last_token.shape)\n",
    "        #print(last_token.shape)\n",
    "        if temperature > 0.0:\n",
    "            probs = torch.softmax(last_token / temperature, dim = -1)\n",
    "            next_token = sample_top_p(probs, top_p).item()\n",
    "            #print(next_token)\n",
    "\n",
    "        else:\n",
    "            next_token = torch.argmax(last_token).item()\n",
    "            print(next_token,last_token[next_token])\n",
    "        #app_res = torch.argmax(last_token).item()\n",
    "        token_string.append(next_token)\n",
    "        #print(test_tokens)\n",
    "        if(next_token ==2):\n",
    "            break\n",
    "    res = generator.tokenizer.decode(token_string)\n",
    "    return res, token_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a626321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (1) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m this_test \u001b[38;5;241m=\u001b[39m prompt_tokens[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m18\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m res, string \u001b[38;5;241m=\u001b[39m \u001b[43mfetus_generate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthis_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m, in \u001b[0;36mfetus_generate_sentence\u001b[0;34m(model, token_string, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     12\u001b[0m     test_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((test_in, padding), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(test_in.shape, i)  \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m test_res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_res\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m last_token \u001b[38;5;241m=\u001b[39m test_res\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/projectnb/textconv/llama/student_models.py:232\u001b[0m, in \u001b[0;36mMemoryBaby.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    231\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_mem(x))\n\u001b[0;32m--> 232\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, y), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    234\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/projectnb/textconv/llama/student_models.py:83\u001b[0m, in \u001b[0;36mDimMemory.forward\u001b[0;34m(self, input1)\u001b[0m\n\u001b[1;32m     81\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(linear_layer(input1))\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m#print(y.shape, x.shape)\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maz,a...yz->a...y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Apply the final linear layer for output\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(x) \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (1) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "this_test = prompt_tokens[0][:18]\n",
    "\n",
    "res, string = fetus_generate_sentence(model,this_test,max_tokens=10,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a96a7581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] what is the recipe of mayonnaise? [/INST] materials materials materials materials materials materials materials materials materials materials'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "525d25cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 518,\n",
       " 25580,\n",
       " 29962,\n",
       " 825,\n",
       " 338,\n",
       " 278,\n",
       " 9522,\n",
       " 412,\n",
       " 310,\n",
       " 1122,\n",
       " 11586,\n",
       " 895,\n",
       " 29973,\n",
       " 518,\n",
       " 29914,\n",
       " 25580,\n",
       " 29962]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens[0][:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b0deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
